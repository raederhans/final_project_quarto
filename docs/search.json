[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 550: Yu Qiushi Final Project",
    "section": "",
    "text": "Abstract\n\n\n\nThis project aims to analyze the relationship between crime rates, crime types (and their timing), housing prices, and racial demographics in Philadelphia. The goal is to determine if patterns exist between these variables and whether factors like racial distribution or housing characteristics (room numbers, specific room types) are linked to crime rates or specific crime types. The analysis will include a focus on when crimes are most likely to occur (on month basis) to uncover temporal trends in criminal activity.\n\n\nIt is further centered around these questions:\n\nIs there a correlation between crime rates and racial demographics in Philadelphia?\nDo certain crime types have stronger links to housing prices or characteristics?\nAre there temporal patterns in criminal activity that vary by crime type?\n\nData Sets used in this project\n\nCrime Data:- Source: Open Data Philly Crime Incident API\nHousing Data: - Source: Carto Property Registration API\nDemographic Data: - Source:Census API\nGeographic Data - Source: Census API downloaded, and through RStudio script as well",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Background",
    "section": "",
    "text": "Background\nPhiladelphia have being well known for its unsafety, crime and chaos at especially the northern part. But we do want to question, does crime affect all people? or some races and disadvantaged groups will suffer more. This project hence tries to investigate the relationship between house prices, crime rate and race distribution across Philadelphia. Which particularly, it ended up find out the crime attack all people and all regions equally, but disadvantaged groups do more likely to be affected. Yet, it also highlights the crime need more social indicators other than property features to investigate their distributions.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap"
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Greetings my Friends!\n\n\nI am Yu Qiushi, one who loves capybara so much\nI am the founder of a Design and Planning company, I also teach, analyze and simply do nothing when I spare, Greetings!\nThumbs up if you like capybara, this project will serve Philadelphia, but also them.\nPlease contact me via email if you are interested",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007"
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");"
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium."
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/Project File.html",
    "href": "analysis/Project File.html",
    "title": "Final Project —— Housing Prices, Crime rate and Racial Geography In Philadelphia",
    "section": "",
    "text": "# This project is the crime project created by Yu Qiushi, il la tout fait!\n\n# please be aware that maybe not all interactive function works properly with the website, then please go through the local file they work perfectly there\n\n\n---\ntitle: \"Project Analysis\"\nformat: \n  html:\n    code-fold: true\n    server: shiny\n    custom-chrome: true\n    theme: cosmo\n    highlight-style: github\nexecute:\n  echo: true\n  eval: true\n  warning: false\n  enabled: true\njupyter: python3\n---\n\nimport panel as pn\npn.extension('vega', 'plotly', 'bokeh')\n\n\n  Cell In[2], line 1\n    ---\n       ^\nSyntaxError: invalid syntax\n\n\n\n\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport altair as alt \nimport holoviews as hv\nimport seaborn as sns\nimport hvplot.pandas\nimport geopandas as gpd\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport geoviews as gv\nimport geoviews.tile_sources as gvts\nimport folium\nimport xyzservices\nfrom folium.plugins import TimestampedGeoJson\nimport osmnx as ox\nimport pandana as pnda\nfrom pandana.loaders import osm\nimport requests\nfrom bs4 import BeautifulSoup\n%matplotlib inline\nfrom folium import GeoJson\nimport panel as pn\nimport warnings\nimport holoviews as hv\nimport hvplot.pandas\nhv.extension('bokeh')\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", message=\"the convert_dtype parameter is deprecated\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\nprint(\"Current working directory:\", os.getcwd())\n\nCurrent working directory: C:\\Users\\44792\\Desktop\\essay help master\\5500 MUSA Python\\24fall-python-final-yu_qiushi_final_project\\final\\final_project_quarto\\analysis\n\n\n\nimport os\nimport geopandas as gpd\n\n# Define the path (absolute path for debugging)\nphiladelphia_range_path = r\"C:\\Users\\44792\\Desktop\\essay help master\\5500 MUSA Python\\24fall-python-final-yu_qiushi_final_project\\final\\final_project_quarto\\analysis\\final_data\\city_limits_3857.geojson\"\n\n# Check file existence\nprint(\"File exists:\", os.path.exists(philadelphia_range_path))\n\n# Read the file if it exists\nif os.path.exists(philadelphia_range_path):\n    city_limits = gpd.read_file(philadelphia_range_path)\n\n    # Check and transform CRS\n    if city_limits.crs.to_string() != \"EPSG:4326\":\n        city_limits = city_limits.to_crs(epsg=4326)\n    print(city_limits.crs)\nelse:\n    print(\"File not found. Please verify the path.\")\n\nFile exists: True\nEPSG:4326\n\n\n\ncity_limits.explore()\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nbase_url = \"https://phl.carto.com/api/v2/sql\"\nsql_query = \"\"\"\nSELECT * FROM incidents_part1_part2\nWHERE dispatch_date &gt;= '2022-01-01' AND dispatch_date &lt;= '2024-12-23'\n\"\"\"\nparams = {\n    'q': sql_query\n}\n\nresponse = requests.get(base_url, params=params)\ndata = response.json()\n\ncrime = pd.DataFrame(data['rows'])\n\n\nprint(crime.head())\n\nprint(len(crime))\n\n   cartodb_id                                           the_geom  \\\n0           2  0101000020E6100000A51C8299A5C752C006342AD3DCFF...   \n1           4  0101000020E6100000F9245E3B64CC52C0B7195D940FF6...   \n2           7  0101000020E6100000118A52E7F6C052C0CFF41263190C...   \n3         123  0101000020E6100000E1F9FB7B5FC552C0159C0B6D4A02...   \n4         126  0101000020E6100000D1CCD5875CCA52C014B723FFC005...   \n\n                                the_geom_webmercator  objectid dc_dist psa  \\\n0  0101000020110F0000F80DE2A145E65FC1E5EC7592BE8F...       114      25   3   \n1  0101000020110F00000426B7CE54EE5FC1C5E06D37E284...       116      01   1   \n2  0101000020110F00006728CED7EBDA5FC169DB64F8519D...       119      08   2   \n3  0101000020110F00009D28D4D968E25FC13CD5C3D06F92...        96      15   1   \n4  0101000020110F00002F28E30AE2EA5FC10090A3314796...        99      14   1   \n\n     dispatch_date_time dispatch_date dispatch_time  hour        dc_key  \\\n0  2023-03-11T17:12:00Z    2023-03-11      12:12:00  12.0  202325014482   \n1  2023-03-11T18:31:00Z    2023-03-11      13:31:00  13.0  202301004597   \n2  2023-03-11T22:13:00Z    2023-03-11      17:13:00  17.0  202308008412   \n3  2023-03-11T12:42:00Z    2023-03-11      07:42:00   7.0  202315017366   \n4  2023-03-12T00:54:00Z    2023-03-11      19:54:00  19.0  202314012625   \n\n              location_block ucr_general   text_general_code    point_x  \\\n0    3300 BLOCK HARTVILLE ST         300  Robbery No Firearm -75.119482   \n1       2400 BLOCK S 28TH ST         600  Theft from Vehicle -75.193618   \n2  9800 BLOCK Roosevelt Blvd         600              Thefts -75.015070   \n3      4700 BLOCK GRISCOM ST         600              Thefts -75.083953   \n4        5500 BLOCK BLOYD ST         300  Robbery No Firearm -75.161898   \n\n     point_y  \n0  39.998927  \n1  39.922350  \n2  40.094525  \n3  40.017896  \n4  40.044952  \n477568\n\n\n\nfrom shapely import wkb\n\ncrime['geometry'] = crime['the_geom'].apply(wkb.loads)\n\ncrime_gdf = gpd.GeoDataFrame(crime, geometry='geometry', crs=\"EPSG:4326\")\n\nif crime_gdf.crs != city_limits.crs:\n    crime_gdf = crime_gdf.to_crs(city_limits.crs)\n\nprint(\"Columns in the crime GeoDataFrame:\", crime_gdf.columns.tolist())\nprint(crime_gdf.head())\n\nColumns in the crime GeoDataFrame: ['cartodb_id', 'the_geom', 'the_geom_webmercator', 'objectid', 'dc_dist', 'psa', 'dispatch_date_time', 'dispatch_date', 'dispatch_time', 'hour', 'dc_key', 'location_block', 'ucr_general', 'text_general_code', 'point_x', 'point_y', 'geometry']\n   cartodb_id                                           the_geom  \\\n0           2  0101000020E6100000A51C8299A5C752C006342AD3DCFF...   \n1           4  0101000020E6100000F9245E3B64CC52C0B7195D940FF6...   \n2           7  0101000020E6100000118A52E7F6C052C0CFF41263190C...   \n3         123  0101000020E6100000E1F9FB7B5FC552C0159C0B6D4A02...   \n4         126  0101000020E6100000D1CCD5875CCA52C014B723FFC005...   \n\n                                the_geom_webmercator  objectid dc_dist psa  \\\n0  0101000020110F0000F80DE2A145E65FC1E5EC7592BE8F...       114      25   3   \n1  0101000020110F00000426B7CE54EE5FC1C5E06D37E284...       116      01   1   \n2  0101000020110F00006728CED7EBDA5FC169DB64F8519D...       119      08   2   \n3  0101000020110F00009D28D4D968E25FC13CD5C3D06F92...        96      15   1   \n4  0101000020110F00002F28E30AE2EA5FC10090A3314796...        99      14   1   \n\n     dispatch_date_time dispatch_date dispatch_time  hour        dc_key  \\\n0  2023-03-11T17:12:00Z    2023-03-11      12:12:00  12.0  202325014482   \n1  2023-03-11T18:31:00Z    2023-03-11      13:31:00  13.0  202301004597   \n2  2023-03-11T22:13:00Z    2023-03-11      17:13:00  17.0  202308008412   \n3  2023-03-11T12:42:00Z    2023-03-11      07:42:00   7.0  202315017366   \n4  2023-03-12T00:54:00Z    2023-03-11      19:54:00  19.0  202314012625   \n\n              location_block ucr_general   text_general_code    point_x  \\\n0    3300 BLOCK HARTVILLE ST         300  Robbery No Firearm -75.119482   \n1       2400 BLOCK S 28TH ST         600  Theft from Vehicle -75.193618   \n2  9800 BLOCK Roosevelt Blvd         600              Thefts -75.015070   \n3      4700 BLOCK GRISCOM ST         600              Thefts -75.083953   \n4        5500 BLOCK BLOYD ST         300  Robbery No Firearm -75.161898   \n\n     point_y                    geometry  \n0  39.998927  POINT (-75.11948 39.99893)  \n1  39.922350  POINT (-75.19362 39.92235)  \n2  40.094525  POINT (-75.01507 40.09452)  \n3  40.017896  POINT (-75.08395 40.01790)  \n4  40.044952  POINT (-75.16190 40.04495)  \n\n\n\nhv.extension('bokeh')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\ncrime_gdf['dispatch_date'] = pd.to_datetime(crime_gdf['dispatch_date'])\n\ncrime_gdf['year_month'] = crime_gdf['dispatch_date'].dt.to_period('M').astype(str)\n\ncrime_monthly = crime_gdf.groupby(['text_general_code', 'year_month']).size().reset_index(name='count')\n\n\nunique_crimes = crime_gdf['text_general_code'].unique()\nprint(f\"Unique crime types ({len(unique_crimes)}):\", unique_crimes)\n\nUnique crime types (32): ['Robbery No Firearm' 'Theft from Vehicle' 'Thefts'\n 'Aggravated Assault Firearm' 'Aggravated Assault No Firearm' 'Rape'\n 'Burglary Residential' 'Robbery Firearm' 'Burglary Non-Residential'\n 'Other Assaults' 'Vandalism/Criminal Mischief'\n 'Narcotic / Drug Law Violations' 'Fraud'\n 'Offenses Against Family and Children' 'Weapon Violations'\n 'All Other Offenses' 'DRIVING UNDER THE INFLUENCE'\n 'Other Sex Offenses (Not Commercialized)' 'Receiving Stolen Property'\n 'Liquor Law Violations' 'Arson' 'Disorderly Conduct' 'Embezzlement'\n 'Prostitution and Commercialized Vice' 'Forgery and Counterfeiting'\n 'Public Drunkenness' 'Vagrancy/Loitering' 'Gambling Violations'\n 'Motor Vehicle Theft' 'Homicide - Criminal' 'Homicide - Justifiable'\n 'Homicide - Gross Negligence']\n\n\n\n#here I reclassify the category of crime so we wont have to test so many case later\n\ncrime_categories = {\n    'Violent Crime': [\n        'Robbery No Firearm', 'Robbery Firearm', 'Aggravated Assault Firearm', \n        'Aggravated Assault No Firearm', 'Homicide - Criminal', 'Homicide - Justifiable', 'Homicide - Gross Negligence'\n    ],\n    'Property Crime': [\n        'Theft from Vehicle', 'Thefts', 'Burglary Residential', 'Burglary Non-Residential', \n        'Motor Vehicle Theft', 'Arson', 'Receiving Stolen Property', 'Vandalism/Criminal Mischief'\n    ],\n    'Drug/Alcohol Crime': [\n        'Narcotic / Drug Law Violations', 'DRIVING UNDER THE INFLUENCE', 'Liquor Law Violations', \n        'Public Drunkenness'\n    ],\n    'Sexual Offenses': [\n        'Rape', 'Other Sex Offenses (Not Commercialized)', 'Prostitution and Commercialized Vice'\n    ],\n    'Miscellaneous': [\n        'Fraud', 'Offenses Against Family and Children', 'Weapon Violations', 'Disorderly Conduct', \n        'Embezzlement', 'Forgery and Counterfeiting', 'Vagrancy/Loitering', 'Gambling Violations', 'All Other Offenses'\n    ]\n}\n\n\ndef map_crime_category(crime_type):\n    for category, crimes in crime_categories.items():\n        if crime_type in crimes:\n            return category\n    return 'Other'\n\ncrime_gdf['crime_category'] = crime_gdf['text_general_code'].apply(map_crime_category)\n\ncrime_monthly = crime_gdf.groupby(['crime_category', 'year_month']).size().reset_index(name='count')\n\ncrime_gdf = crime_gdf[crime_gdf['geometry'].notnull()]\ncrime_gdf = crime_gdf[crime_gdf['geometry'].astype(object).apply(lambda geom: geom.is_valid if geom else False)]\n\ncrime_map = crime_gdf.hvplot.points(\n    'geometry.x', 'geometry.y',\n    geo=True,\n    tiles='OSM',\n    groupby='year_month',\n    color='crime_category',\n    title='Crime Events by Month and Category',\n    size=5,\n    alpha=0.6,\n    height=600,\n    width=800\n)\n\nC:\\Users\\44792\\miniforge3\\envs\\musa-550-fall-2023\\lib\\site-packages\\geopandas\\geoseries.py:751: UserWarning: GeoSeries.notna() previously returned False for both missing (None) and empty geometries. Now, it only returns False for missing values. Since the calling GeoSeries contains empty geometries, the result has changed compared to previous versions of GeoPandas.\nGiven a GeoSeries 's', you can use '~s.is_empty & s.notna()' to get back the old behaviour.\n\nTo further ignore this warning, you can do: \nimport warnings; warnings.filterwarnings('ignore', 'GeoSeries.notna', UserWarning)\n  return self.notna()\n\n\n\nimport holoviews as hv\nhv.extension('bokeh')\n\ncrime_monthly = crime_gdf.groupby(['text_general_code', 'year_month']).size().reset_index(name='count')\ncrime_plot = crime_monthly.hvplot.line(\n    x='year_month', \n    y='count', \n    by='text_general_code',\n    title='Monthly Crime Trends by Type', \n    xlabel='Month', \n    ylabel='Number of Crimes', \n    legend='top_left', \n    width=1000, \n    height=500\n)\ncrime_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\ncity_limits_layer = city_limits.hvplot.polygons(\n    geo=True,\n    tiles=\"CartoLight\",  # Set a light gray basemap\n    line_width=2,\n    line_color='black',\n    fill_alpha=0.1,\n    height=400,\n    width=600\n)\n\ncity_limits_layer\n\n\n\n\n\n  \n\n\n\n\n\nfrom bokeh.palettes import inferno\n\ncrime_categories = ['Violent Crime', 'Property Crime', 'Drug/Alcohol Crime', 'Sexual Offenses', 'Miscellaneous']\ncrime_colors = {category: inferno(len(crime_categories))[i] for i, category in enumerate(crime_categories)}\n\n\n#now lets visualize some data!\ncity_limits_layer = city_limits.hvplot.polygons(\n    geo=True,\n    tiles=None,\n    line_width=2,\n    line_color='red',\n    fill_alpha=0,\n    height=700,\n    width=900\n)\n\n# Create a Panel interactive dashboard\nmonth_slider = pn.widgets.IntSlider(name='Month', start=0, end=len(crime_gdf['year_month'].unique()) - 1, step=1, value=0)\nmonth_list = sorted(crime_gdf['year_month'].unique())\n\n@pn.depends(month_slider.param.value, watch=True)\ndef update_month(selected_index):\n    return month_list[selected_index]\n\ntype_selector = pn.widgets.CheckBoxGroup(\n    name='Crime Types',\n    value=list(crime_colors.keys()),\n    options=list(crime_colors.keys())\n)\n\n@pn.depends(month_slider.param.value, type_selector.param.value)\ndef update_map(selected_index, selected_types):\n    selected_month = month_list[selected_index]\n    filtered_data = crime_gdf[\n        (crime_gdf['year_month'] == selected_month) & \n        (crime_gdf['crime_category'].isin(selected_types))\n    ]\n    crime_layer = filtered_data.hvplot.points(\n        'geometry.x', 'geometry.y',\n        geo=True,\n        tiles=\"CartoLight\",\n        color='crime_category',\n        cmap=crime_colors,\n        size=5,\n        alpha=0.6,\n        height=700,\n        width=900\n    )\n    return city_limits_layer * crime_layer\n\n@pn.depends(month_slider.param.value, type_selector.param.value)\ndef update_bar_chart(selected_index, selected_types):\n    selected_month = month_list[selected_index]  # 从索引获取月份\n    filtered_data = crime_gdf[\n        (crime_gdf['year_month'] == selected_month) & \n        (crime_gdf['crime_category'].isin(selected_types))\n    ]\n    monthly_counts = filtered_data.groupby('crime_category').size().reset_index(name='count')\n    bar_chart = alt.Chart(monthly_counts).mark_bar().encode(\n        x=alt.X('crime_category', sort='-y', title='Crime Category'),\n        y=alt.Y('count', title='Count'),\n        color=alt.Color('crime_category', scale=alt.Scale(domain=list(crime_colors.keys()), range=list(crime_colors.values())))\n    ).properties(\n        title=f\"Crime Categories for {selected_month}\",\n        width=400,\n        height=250\n    )\n    return bar_chart\n\n@pn.depends(type_selector.param.value)\ndef update_line_chart(selected_types):\n    filtered_data = crime_gdf[crime_gdf['crime_category'].isin(selected_types)]\n    monthly_counts = filtered_data.groupby(['year_month', 'crime_category']).size().reset_index(name='count')\n    line_chart = alt.Chart(monthly_counts).mark_line(point=True).encode(\n        x=alt.X('year_month:T', title='Month'),\n        y=alt.Y('count:Q', title='Count'),\n        color=alt.Color('crime_category:N', scale=alt.Scale(domain=list(crime_colors.keys()), range=list(crime_colors.values())))\n    ).properties(\n        title=\"Crime Trends by Month\",\n        width=400,\n        height=250\n    )\n    return line_chart\n\n# Combine bar chart and line chart into a single vertical panel\n@pn.depends(month_slider.param.value, type_selector.param.value)\ndef combined_chart(selected_index, selected_types):\n    bar = update_bar_chart(selected_index, selected_types)\n    line = update_line_chart(selected_types)\n    return pn.Column(pn.pane.Vega(bar), pn.pane.Vega(line), sizing_mode='stretch_width')\n\n# Update layout to include combined charts in the map\ncrime_dashboard = pn.Column(\n    \"## Crime Dashboard\",\n    month_slider,\n    type_selector,\n    pn.Row(\n        update_map,\n        combined_chart,\n        sizing_mode='stretch_width'\n    )\n)\n\ncrime_dashboard.servable()\n\n\n\n\nWARNING:param.main: VegaPlot was not imported on instantiation and may not render in a notebook. Restart the notebook kernel and ensure you load it as part of the extension using:\n\npn.extension('vega')\n\n\n\n\n\n\n\n  \n\n\n\n\n\n####Social Data Part!####\n\n\nimport cenpy as cny\n\n#cny.set_sitekey(\"86e9f13d3e07b9344204c7b748ce3feef9772df5\")\n\n\nacs_dataset = \"ACSDT5Y2023\"  # ACS 5-Year dataset for 2023\nconn = cny.products.APIConnection(acs_dataset)\n\nvariables_df = conn.variables\nprint(variables_df.head())\n\n                                                          label  \\\nfor                                Census API FIPS 'for' clause   \nin                                  Census API FIPS 'in' clause   \nucgid                Uniform Census Geography Identifier clause   \nB24022_060E   Estimate!!Total:!!Female:!!Service occupations...   \nB19001B_014E             Estimate!!Total:!!$100,000 to $124,999   \n\n                                                        concept predicateType  \\\nfor                          Census API Geography Specification      fips-for   \nin                           Census API Geography Specification       fips-in   \nucgid                        Census API Geography Specification         ucgid   \nB24022_060E   Sex by Occupation and Median Earnings in the P...           int   \nB19001B_014E  Household Income in the Past 12 Months (in 202...           int   \n\n                group limit predicateOnly hasGeoCollectionSupport  \\\nfor               N/A     0          True                     NaN   \nin                N/A     0          True                     NaN   \nucgid             N/A     0          True                    True   \nB24022_060E    B24022     0           NaN                     NaN   \nB19001B_014E  B19001B     0           NaN                     NaN   \n\n                                            attributes required  \nfor                                                NaN      NaN  \nin                                                 NaN      NaN  \nucgid                                              NaN      NaN  \nB24022_060E      B24022_060EA,B24022_060M,B24022_060MA      NaN  \nB19001B_014E  B19001B_014EA,B19001B_014M,B19001B_014MA      NaN  \n\n\n\n#scape some social data here to visualize\nacs_variables = {\n    \"B01001_001E\": \"Total Population\",\n    \"B03002_006E\": \"Asian Alone\",\n    \"B03002_004E\": \"Black or African American Alone\",\n    \"B03003_003E\": \"Hispanic or Latino\",\n    \"B03002_003E\": \"White Alone\",\n    \"B01002_001E\": \"Median Age\",\n    \"B11001_001E\": \"Total Households\",\n    \"B11001_002E\": \"Total Families\",\n    \"B11001_003E\": \"Married-couple Family\",\n    \"B25001_001E\": \"Total Housing Units\",\n    \"B25002_002E\": \"Occupied Housing Units\",\n    \"B25002_003E\": \"Vacant Housing Units\",\n    \"B25003_002E\": \"Owner-occupied Housing Units\",\n    \"B25003_003E\": \"Renter-occupied Housing Units\",\n    \"B19013_001E\": \"Median Household Income\",\n    \"B17010_001E\": \"Poverty Status Universe\",\n    \"B17010_002E\": \"Below Poverty Level\"\n}\n\nacs = cny.products.ACS()\ndata = acs.from_county(\n    county=\"Philadelphia County, PA\",\n    variables=list(acs_variables.keys()),\n    level=\"tract\",\n    return_geometry=True\n)\n\ndata = data.rename(columns=dict(zip(acs_variables.keys(), acs_variables.values())))\n\ndata.to_file(\"Philadelphia_ACS_2023.shp\")\n\nC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_24168\\1812328781.py:32: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n  data.to_file(\"Philadelphia_ACS_2023.shp\")\n\n\n\nsocial_file_path = \"Philadelphia_ACS_2023.shp\"\nacs_gdf = gpd.read_file(social_file_path)\n\nif acs_gdf.crs.to_string() != \"EPSG:4326\":\n    acs_gdf = acs_gdf.to_crs(epsg=4326)\n\nprint(\"CRS:\", acs_gdf.crs)\nprint(\"Columns:\", acs_gdf.columns)\nprint(acs_gdf.head())\n\nacs_gdf.explore()\n\nCRS: EPSG:4326\nColumns: Index(['GEOID', 'Total Popu', 'Median Age', 'White Alon', 'Black or A',\n       'Asian Alon', 'Hispanic o', 'Total Hous', 'Total Fami', 'Married-co',\n       'Poverty St', 'Below Pove', 'Median Hou', 'Total Ho_1', 'Occupied H',\n       'Vacant Hou', 'Owner-occu', 'Renter-occ', 'NAME', 'state', 'county',\n       'tract', 'geometry'],\n      dtype='object')\n         GEOID  Total Popu  Median Age  White Alon  Black or A  Asian Alon  \\\n0  42101009802      6190.0        33.5       580.0      5341.0        35.0   \n1  42101037500      3736.0        31.3      1672.0      1467.0       165.0   \n2  42101021900      1434.0        44.7      1307.0        50.0        24.0   \n3  42101011300      3344.0        28.5        62.0      3229.0         0.0   \n4  42101021800      5141.0        30.9      2822.0      1497.0       247.0   \n\n   Hispanic o  Total Hous  Total Fami  Married-co  ...  Total Ho_1  \\\n0       149.0      2129.0      1467.0       576.0  ...      2371.0   \n1       298.0      1224.0       733.0       547.0  ...      1383.0   \n2        41.0       654.0       307.0       238.0  ...       751.0   \n3        53.0      1102.0       770.0       133.0  ...      1392.0   \n4       178.0      2242.0      1073.0       887.0  ...      2394.0   \n\n   Occupied H  Vacant Hou  Owner-occu  Renter-occ  \\\n0      2129.0       242.0      1593.0       536.0   \n1      1224.0       159.0       722.0       502.0   \n2       654.0        97.0       551.0       103.0   \n3      1102.0       290.0       621.0       481.0   \n4      2242.0       152.0       597.0      1645.0   \n\n                                                NAME  state  county   tract  \\\n0  Census Tract 98.02, Philadelphia County, Penns...     42     101  009802   \n1  Census Tract 375, Philadelphia County, Pennsyl...     42     101  037500   \n2  Census Tract 219, Philadelphia County, Pennsyl...     42     101  021900   \n3  Census Tract 113, Philadelphia County, Pennsyl...     42     101  011300   \n4  Census Tract 218, Philadelphia County, Pennsyl...     42     101  021800   \n\n                                            geometry  \n0  POLYGON ((-75.27547 39.97743, -75.27528 39.977...  \n1  POLYGON ((-75.26551 39.98186, -75.26475 39.982...  \n2  POLYGON ((-75.25438 40.04657, -75.25384 40.046...  \n3  POLYGON ((-75.23947 39.98111, -75.23942 39.981...  \n4  POLYGON ((-75.23807 40.05988, -75.23622 40.061...  \n\n[5 rows x 23 columns]\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nvisualizable_columns = [\n    col for col in acs_gdf.columns if acs_gdf[col].dtype in [float, int]\n]\n\nprint(visualizable_columns)\n\n['Total Popu', 'Median Age', 'White Alon', 'Black or A', 'Asian Alon', 'Hispanic o', 'Total Hous', 'Total Fami', 'Married-co', 'Poverty St', 'Below Pove', 'Median Hou', 'Total Ho_1', 'Occupied H', 'Vacant Hou', 'Owner-occu', 'Renter-occ']\n\n\n\n#Now lets see how social data works!\n\nimport folium\nfrom folium import GeoJson, Tooltip\nimport panel as pn\nimport mapclassify as mc\nimport branca.colormap as cm\n\nclassification_methods = ['Natural Breaks', 'Equal Interval']\nnum_classes_options = list(range(5, 11))\n\ndata_selector = pn.widgets.Select(\n    name=\"Select Data to Visualize\", \n    options=visualizable_columns,\n    value=visualizable_columns[0]\n)\nclassification_selector = pn.widgets.Select(\n    name=\"Select Classification Method\",\n    options=classification_methods,\n    value=classification_methods[0]\n)\nnum_classes_selector = pn.widgets.IntSlider(\n    name=\"Select Number of Classes\",\n    start=5, \n    end=8, \n    step=1, \n    value=5\n)\n\n# data update function\n@pn.depends(\n    data_selector.param.value,\n    classification_selector.param.value,\n    num_classes_selector.param.value\n)\ndef update_map(selected_column, classification_method, num_classes):\n    m = folium.Map(\n        location=[39.9526, -75.1652],\n        zoom_start=12,\n        tiles=\"CartoDB positron\",\n        control_scale=True,\n        width=900,\n        height=600\n    )\n    \n    if classification_method == 'Natural Breaks':\n        classifier = mc.NaturalBreaks(acs_gdf[selected_column], k=num_classes)\n    elif classification_method == 'Equal Interval':\n        classifier = mc.EqualInterval(acs_gdf[selected_column], k=num_classes)\n    \n    acs_gdf['class'] = classifier.yb\n    bins = classifier.bins\n    \n    colormap = cm.LinearColormap(\n        colors=['#f7fcf0', '#c7e9c0', '#7fcdbb', '#41b6c4', '#2c7fb8', '#253494'],\n        vmin=acs_gdf[selected_column].min(),\n        vmax=acs_gdf[selected_column].max(),\n        caption=f\"{selected_column} ({classification_method})\"\n    )\n    \n\n    fill_color_mapping = {i: colormap(v) for i, v in enumerate(bins)}\n\n    for _, row in acs_gdf.iterrows():\n        tooltip_text = (\n            f\"&lt;b&gt;{row['NAME']}&lt;/b&gt;&lt;br&gt;\"\n            f\"{selected_column}: {row[selected_column]:,.2f}\"\n        )\n        GeoJson(\n            row['geometry'],\n            style_function=lambda x, row=row: {\n                \"fillColor\": fill_color_mapping[row['class']],\n                \"color\": \"transparent\", \n                \"weight\": 0,\n                \"fillOpacity\": 0.7,\n            },\n            tooltip=Tooltip(tooltip_text)\n        ).add_to(m)\n    \n    colormap.add_to(m)\n    return m\n\ndashboard = pn.Column(\n    pn.pane.Markdown(\"## Philadelphia ACS Data Visualization\"),\n    data_selector,\n    classification_selector,\n    num_classes_selector,\n    pn.panel(update_map, sizing_mode=\"stretch_width\", height=600)\n)\n\ndashboard.servable()\n\nC:\\Users\\44792\\miniforge3\\envs\\musa-550-fall-2023\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\nfound 0 physical cores &lt; 1\nReturning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n  warnings.warn(\n  File \"C:\\Users\\44792\\miniforge3\\envs\\musa-550-fall-2023\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n    raise ValueError(f\"found {cpu_count_physical} physical cores &lt; 1\")\nC:\\Users\\44792\\miniforge3\\envs\\musa-550-fall-2023\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n  warnings.warn(\n\n\n\n\n\n\n  \n\n\n\n\n\n####Go with the housing data web scraping\n\n\nHousing_API_URL = \"https://phl.carto.com/api/v2/sql\"\n\nstart_date = \"2022-01-01\"\nend_date = \"2024-12-23\"\n\nquery = f\"\"\"\nSELECT *\nFROM opa_properties_public\nWHERE assessment_date &gt;= '{start_date}' AND assessment_date &lt;= '{end_date}'\n\"\"\"\n\nresponse = requests.get(f\"{Housing_API_URL}?q={query}\")\nif response.status_code != 200:\n    raise Exception(f\"API request failed with status {response.status_code}: {response.text}\")\n\n\n#got so many columns that some we wont need later\n\nHousing_data = response.json()\nproperties_df = pd.DataFrame(Housing_data['rows'])\n\nprint(\"Columns in the dataset:\", properties_df.columns)\n\nColumns in the dataset: Index(['cartodb_id', 'the_geom', 'the_geom_webmercator', 'assessment_date',\n       'basements', 'beginning_point', 'book_and_page', 'building_code',\n       'building_code_description', 'category_code',\n       'category_code_description', 'census_tract', 'central_air',\n       'cross_reference', 'date_exterior_condition', 'depth',\n       'exempt_building', 'exempt_land', 'exterior_condition', 'fireplaces',\n       'frontage', 'fuel', 'garage_spaces', 'garage_type',\n       'general_construction', 'geographic_ward', 'homestead_exemption',\n       'house_extension', 'house_number', 'interior_condition', 'location',\n       'mailing_address_1', 'mailing_address_2', 'mailing_care_of',\n       'mailing_city_state', 'mailing_street', 'mailing_zip', 'market_value',\n       'market_value_date', 'number_of_bathrooms', 'number_of_bedrooms',\n       'number_of_rooms', 'number_stories', 'off_street_open',\n       'other_building', 'owner_1', 'owner_2', 'parcel_number', 'parcel_shape',\n       'quality_grade', 'recording_date', 'registry_number', 'sale_date',\n       'sale_price', 'separate_utilities', 'sewer', 'site_type', 'state_code',\n       'street_code', 'street_designation', 'street_direction', 'street_name',\n       'suffix', 'taxable_building', 'taxable_land', 'topography',\n       'total_area', 'total_livable_area', 'type_heater', 'unfinished', 'unit',\n       'utility', 'view_type', 'year_built', 'year_built_estimate', 'zip_code',\n       'zoning', 'pin', 'building_code_new', 'building_code_description_new',\n       'objectid'],\n      dtype='object')\n\n\n\nproperties_df.tail()\n\n\n\n\n\n\n\n\ncartodb_id\nthe_geom\nthe_geom_webmercator\nassessment_date\nbasements\nbeginning_point\nbook_and_page\nbuilding_code\nbuilding_code_description\ncategory_code\n...\nutility\nview_type\nyear_built\nyear_built_estimate\nzip_code\nzoning\npin\nbuilding_code_new\nbuilding_code_description_new\nobjectid\n\n\n\n\n583657\n584047\n0101000020E6100000CA62F43784C452C0553088271300...\n0101000020110F0000DF8AED67F4E05FC1CB865FCEFA8F...\n2024-06-06T16:07:08Z\nD\n417'8 3/4\" NE LEFEVRE\n0000000\nH36\nSEMI/DET 2 STY FRAME\n1\n...\nNone\nI\n1920\nNone\n19137\nRSA5\n1001192329\n32\nTWIN CONVENTIONAL\n563626477\n\n\n583658\n584048\n0101000020E61000002876388229C352C0DAF8E3121703...\n0101000020110F00002E29FC7BA7DE5FC15D1760C65293...\n2024-06-06T16:05:14Z\nNone\n15' 1/2\"SE DITMAN\n0000000\nO30\nROW 2 STY MASONRY\n1\n...\nNone\nI\n1920\nY\n19135\nRSA5\n1001342218\n24\nROW PORCH FRONT\n563626478\n\n\n583659\n584049\n0101000020E61000009638723466CA52C07637567AF002...\n0101000020110F0000C30DA979F2EA5FC11BC631F82793...\n2024-11-16T12:29:40Z\nNone\nSE COR APSLEY ST\nNone\nLB0\nIND. LGHT MFG MASONRY\n14\n...\nNone\nI\n2023\nNone\n19144\nCMX3\n1001681092\n830\nAPARTMENTS - MID RISE\n563626479\n\n\n583660\n584050\n0101000020E61000009638723466CA52C07637567AF002...\n0101000020110F0000C30DA979F2EA5FC11BC631F82793...\n2024-12-14T16:53:16Z\nNone\nSE COR APSLEY ST\nNone\nLB0\nIND. LGHT MFG MASONRY\n10\n...\nNone\nI\n1920\nNone\n19144\nCMX3\n1001681093\n241\nOFFICE BLDNG - Low Rise\n563626480\n\n\n583661\n584051\n0101000020E6100000C75E7D144DC552C04F40CC0FA801...\n0101000020110F0000F891E19649E25FC1DADDADC3BB91...\n2024-06-06T16:05:55Z\nNone\n128'9\" N GILLINGHAM\n0000000\nSR\nVACANT LAND RESIDE &lt; ACRE\n6\n...\nNone\nE\nNone\nNone\n19124\nICMX\n1001579732\nNone\nNone\n563626481\n\n\n\n\n5 rows × 81 columns\n\n\n\n\nfrom shapely import wkb\n\ndef safe_wkb_loads(wkb_str):\n    try:\n        return wkb.loads(bytes.fromhex(wkb_str))\n    except Exception:\n        return None\n\nproperties_df['parsed_geometry'] = properties_df['the_geom'].apply(safe_wkb_loads)\n\nproperties_with_geometry = properties_df[properties_df['parsed_geometry'].notnull()]\n\nprint(f\"Number of properties after filtering: {len(properties_with_geometry)}\")\nprint(properties_with_geometry.tail())\n\nNumber of properties after filtering: 583662\n        cartodb_id                                           the_geom  \\\n583657      584047  0101000020E6100000CA62F43784C452C0553088271300...   \n583658      584048  0101000020E61000002876388229C352C0DAF8E3121703...   \n583659      584049  0101000020E61000009638723466CA52C07637567AF002...   \n583660      584050  0101000020E61000009638723466CA52C07637567AF002...   \n583661      584051  0101000020E6100000C75E7D144DC552C04F40CC0FA801...   \n\n                                     the_geom_webmercator  \\\n583657  0101000020110F0000DF8AED67F4E05FC1CB865FCEFA8F...   \n583658  0101000020110F00002E29FC7BA7DE5FC15D1760C65293...   \n583659  0101000020110F0000C30DA979F2EA5FC11BC631F82793...   \n583660  0101000020110F0000C30DA979F2EA5FC11BC631F82793...   \n583661  0101000020110F0000F891E19649E25FC1DADDADC3BB91...   \n\n             assessment_date basements        beginning_point book_and_page  \\\n583657  2024-06-06T16:07:08Z         D  417'8 3/4\" NE LEFEVRE       0000000   \n583658  2024-06-06T16:05:14Z      None      15' 1/2\"SE DITMAN       0000000   \n583659  2024-11-16T12:29:40Z      None       SE COR APSLEY ST          None   \n583660  2024-12-14T16:53:16Z      None       SE COR APSLEY ST          None   \n583661  2024-06-06T16:05:55Z      None    128'9\" N GILLINGHAM       0000000   \n\n       building_code  building_code_description category_code  ... view_type  \\\n583657         H36         SEMI/DET 2 STY FRAME            1   ...         I   \n583658         O30            ROW 2 STY MASONRY            1   ...         I   \n583659         LB0        IND. LGHT MFG MASONRY            14  ...         I   \n583660         LB0        IND. LGHT MFG MASONRY            10  ...         I   \n583661         SR     VACANT LAND RESIDE &lt; ACRE            6   ...         E   \n\n       year_built year_built_estimate zip_code zoning         pin  \\\n583657       1920                None    19137   RSA5  1001192329   \n583658       1920                   Y    19135   RSA5  1001342218   \n583659       2023                None    19144   CMX3  1001681092   \n583660       1920                None    19144   CMX3  1001681093   \n583661       None                None    19124   ICMX  1001579732   \n\n        building_code_new  building_code_description_new   objectid  \\\n583657                 32              TWIN CONVENTIONAL  563626477   \n583658                 24                ROW PORCH FRONT  563626478   \n583659                830          APARTMENTS - MID RISE  563626479   \n583660                241        OFFICE BLDNG - Low Rise  563626480   \n583661               None                           None  563626481   \n\n                                     parsed_geometry  \n583657  POINT (-75.07056998124895 40.00058454656452)  \n583658  POINT (-75.04940848840545 40.02414165622186)  \n583659  POINT (-75.16248809008025 40.02296380243108)  \n583660  POINT (-75.16248809008025 40.02296380243108)  \n583661  POINT (-75.08282959216295 40.01294133637622)  \n\n[5 rows x 82 columns]\n\n\n\ncolumns_to_drop_housing = [\n    'beginning_point', 'book_and_page', 'building_code',\n    'building_code_description', 'category_code',\n    'category_code_description', 'cross_reference', 'date_exterior_condition', 'depth',\n    'exempt_building', 'exempt_land', 'exterior_condition',\n    'general_construction', 'geographic_ward', 'homestead_exemption',\n    'house_extension', 'location',\n    'mailing_address_1', 'mailing_address_2', 'mailing_care_of',\n    'mailing_city_state', 'mailing_street', 'mailing_zip',\n    'other_building', 'owner_1', 'owner_2', 'parcel_number', 'parcel_shape', \n    'registry_number', 'separate_utilities', 'sewer', 'site_type', 'state_code',\n    'street_code', 'street_designation', 'street_direction', 'street_name',\n    'suffix', 'taxable_building', 'taxable_land', 'topography', 'unfinished', 'unit',\n    'utility', 'view_type', 'zip_code', \n    'zoning', 'pin', 'building_code_new', 'building_code_description_new'\n]\n\ncolumns_present_housing = [col for col in columns_to_drop_housing if col in properties_df.columns]\ncleaned_properties_df = properties_df.drop(columns=columns_present_housing, axis=1)\n\nprint(\"Remaining columns in the cleaned dataset:\")\nprint(cleaned_properties_df.columns)\n\nRemaining columns in the cleaned dataset:\nIndex(['cartodb_id', 'the_geom', 'the_geom_webmercator', 'assessment_date',\n       'basements', 'census_tract', 'central_air', 'fireplaces', 'frontage',\n       'fuel', 'garage_spaces', 'garage_type', 'house_number',\n       'interior_condition', 'market_value', 'market_value_date',\n       'number_of_bathrooms', 'number_of_bedrooms', 'number_of_rooms',\n       'number_stories', 'off_street_open', 'quality_grade', 'recording_date',\n       'sale_date', 'sale_price', 'total_area', 'total_livable_area',\n       'type_heater', 'year_built', 'year_built_estimate', 'objectid',\n       'parsed_geometry'],\n      dtype='object')\n\n\n\ncleaned_properties_gdf = gpd.GeoDataFrame(\n    cleaned_properties_df,\n    geometry='parsed_geometry',\n    crs=\"EPSG:4326\"\n)\n\nprint(f\"Number of properties after filtering and column dropping: {len(cleaned_properties_gdf)}\")\ncleaned_properties_gdf.head()\n\nNumber of properties after filtering and column dropping: 583662\n\n\n\n\n\n\n\n\n\ncartodb_id\nthe_geom\nthe_geom_webmercator\nassessment_date\nbasements\ncensus_tract\ncentral_air\nfireplaces\nfrontage\nfuel\n...\nrecording_date\nsale_date\nsale_price\ntotal_area\ntotal_livable_area\ntype_heater\nyear_built\nyear_built_estimate\nobjectid\nparsed_geometry\n\n\n\n\n0\n1\n0101000020E6100000DBD817B5A3CB52C0B69AFD4035FF...\n0101000020110F000090B604C90DED5FC1CB2D85CC048F...\n2024-06-06T16:10:03Z\nNone\n169\nNone\nNaN\n14.0\nNone\n...\n2071-12-27T05:00:00Z\n1971-12-27T05:00:00Z\n1.0\n639.0\nNaN\nNone\nNone\nNone\n563042662\nPOINT (-75.18187 39.99381)\n\n\n1\n2\n0101000020E61000006BFC9A49A4CB52C058DAA6D48BFE...\n0101000020110F0000E00B48C50EED5FC1019687FC488E...\n2024-06-06T16:08:52Z\nNone\n151\nNone\nNaN\n14.0\nNone\n...\n2071-12-27T05:00:00Z\n2071-12-27T05:00:00Z\n1.0\n674.0\nNaN\nNone\nNone\nNone\n563042663\nPOINT (-75.18190 39.98864)\n\n\n2\n3\n0101000020E6100000111C0D6E98CF52C0573761511EF6...\n0101000020110F00000FE20CFFC5F35FC1FBFAC589F284...\n2024-06-06T16:12:21Z\nNone\n64\nNone\nNaN\n72.0\nNone\n...\n2071-12-17T05:00:00Z\n2071-12-17T05:00:00Z\n1.0\n6636.0\n2400.0\nNone\n1925\nNone\n563042664\nPOINT (-75.24368 39.92280)\n\n\n3\n4\n0101000020E61000009151A5AE7ACB52C0B624691A9BFE...\n0101000020110F00007D008E19C8EC5FC11B9681EA598E...\n2024-06-06T16:09:40Z\nNone\n151\nNone\nNaN\n16.0\nNone\n...\n2071-12-16T05:00:00Z\n2071-12-16T05:00:00Z\n1.0\n992.0\nNaN\nNone\nNone\nNone\n563042665\nPOINT (-75.17936 39.98911)\n\n\n4\n5\n0101000020E61000005A33DE5D9ECA52C0F73C1F77F5FD...\n0101000020110F0000EB7628DF51EB5FC120B8F14FA28D...\n2024-06-06T16:08:58Z\nNone\n152\nNone\nNaN\n15.0\nNone\n...\n2071-12-14T05:00:00Z\n2071-12-14T05:00:00Z\n1.0\n1122.0\nNaN\nNone\nNone\nNone\n563042666\nPOINT (-75.16592 39.98405)\n\n\n\n\n5 rows × 32 columns\n\n\n\n\n#from above, we have cleared up the dataset to what we want, and can see there are 583thousand more data\n\n\nimport datashader as ds\nimport datashader.transfer_functions as tf\nfrom datashader.utils import lnglat_to_meters\nimport geopandas as gpd\n\nfrom datashader.colors import Greys9, viridis, inferno\nfrom colorcet import fire\n\n\n#here trying to show how did the data points spread out bounded by city limits\nphiladelphia_range_path = \"final_data/city_limits_3857.geojson\"\ncity_limits = gpd.read_file(philadelphia_range_path)\n\nif city_limits.crs.to_string() != \"EPSG:4326\":\n    city_limits = city_limits.to_crs(epsg=4326)\n\ncleaned_properties_gdf = cleaned_properties_gdf.to_crs(epsg=3857)\ncity_limits = city_limits.to_crs(epsg=3857)\n\ncity_bounds = city_limits.total_bounds\nx_range, y_range = (city_bounds[0], city_bounds[2]), (city_bounds[1], city_bounds[3])\n\ncleaned_properties_gdf[\"x\"] = cleaned_properties_gdf.geometry.x\ncleaned_properties_gdf[\"y\"] = cleaned_properties_gdf.geometry.y\n\ncanvas = ds.Canvas(\n    plot_width=800,\n    plot_height=600,\n    x_range=(city_limits.total_bounds[0], city_limits.total_bounds[2]),  # x 范围\n    y_range=(city_limits.total_bounds[1], city_limits.total_bounds[3]),  # y 范围\n)\n\nagg = canvas.points(cleaned_properties_gdf, \"x\", \"y\", agg=ds.count())\n\nimage = tf.shade(agg, cmap=[\"lightblue\", \"blue\", \"darkblue\"], how=\"log\")\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\nax.imshow(\n    image.to_pil(),\n    extent=(\n        city_limits.total_bounds[0],  # xmin\n        city_limits.total_bounds[2],  # xmax\n        city_limits.total_bounds[1],  # ymin\n        city_limits.total_bounds[3],  # ymax\n    ),\n    origin=\"upper\",\n)\n\ncity_limits.plot(ax=ax, edgecolor=\"red\", linewidth=1, facecolor=\"none\")\n\nax.set_title(\"Philadelphia Housing Properties Distribution (Sampled)\", fontsize=16)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n#now lets explore the housing data by month and by census tract\n\n\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport os\n\nos.chdir(r\"C:\\Users\\44792\\Desktop\\essay help master\\5500 MUSA Python\\24fall-python-final-yu_qiushi_final_project\")\nprint(f\"Changed working directory to: {os.getcwd()}\")\n\nrelative_path = \"final/Philadelphia_ACS_2023.shp\"\n\ntry:\n    census_gdf = gpd.read_file(relative_path)\nexcept FileNotFoundError:\n    raise FileNotFoundError(f\"The file at {relative_path} does not exist. Please check the path.\")\n\nif census_gdf.crs != \"EPSG:3857\":\n    census_gdf = census_gdf.to_crs(epsg=3857)\n\nprint(f\"Census Tract data loaded with {len(census_gdf)} rows.\")\n\nChanged working directory to: C:\\Users\\44792\\Desktop\\essay help master\\5500 MUSA Python\\24fall-python-final-yu_qiushi_final_project\nCensus Tract data loaded with 384 rows.\n\n\n\ncensus_gdf.tail()\n\n\n\n\n\n\n\n\nGEOID\nTotal Popu\nMedian Age\nWhite Alon\nBlack or A\nAsian Alon\nHispanic o\nTotal Hous\nTotal Fami\nMarried-co\n...\nTotal Ho_1\nOccupied H\nVacant Hou\nOwner-occu\nRenter-occ\nNAME\nstate\ncounty\ntract\ngeometry\n\n\n\n\n379\n42101035602\n3661.0\n42.1\n2518.0\n73.0\n707.0\n307.0\n1252.0\n999.0\n834.0\n...\n1284.0\n1252.0\n32.0\n1022.0\n230.0\nCensus Tract 356.02, Philadelphia County, Penn...\n42\n101\n035602\nPOLYGON ((-8355008.800 4881749.700, -8354858.9...\n\n\n380\n42101035601\n5272.0\n55.4\n4069.0\n140.0\n887.0\n127.0\n2155.0\n1344.0\n964.0\n...\n2404.0\n2155.0\n249.0\n1398.0\n757.0\nCensus Tract 356.01, Philadelphia County, Penn...\n42\n101\n035601\nPOLYGON ((-8354256.940 4879308.850, -8354227.1...\n\n\n381\n42101033102\n4048.0\n34.3\n3107.0\n299.0\n31.0\n438.0\n1556.0\n970.0\n651.0\n...\n1760.0\n1556.0\n204.0\n935.0\n621.0\nCensus Tract 331.02, Philadelphia County, Penn...\n42\n101\n033102\nPOLYGON ((-8352727.080 4872434.270, -8352711.8...\n\n\n382\n42101034801\n4600.0\n44.6\n3812.0\n220.0\n201.0\n342.0\n1969.0\n1215.0\n761.0\n...\n2006.0\n1969.0\n37.0\n922.0\n1047.0\nCensus Tract 348.01, Philadelphia County, Penn...\n42\n101\n034801\nPOLYGON ((-8351724.540 4874706.750, -8351706.9...\n\n\n383\n42101036201\n5153.0\n38.5\n4195.0\n308.0\n0.0\n419.0\n1981.0\n1257.0\n839.0\n...\n2153.0\n1981.0\n172.0\n1327.0\n654.0\nCensus Tract 362.01, Philadelphia County, Penn...\n42\n101\n036201\nPOLYGON ((-8348332.630 4877819.050, -8348033.1...\n\n\n\n\n5 rows × 23 columns\n\n\n\n\n#here is trying to join the property data with the social data together, and i experiment about to show sale prices\nproperties_with_tract = gpd.sjoin(\n    cleaned_properties_gdf, census_gdf, how=\"inner\", predicate=\"intersects\"\n)\n\nproperties_with_tract[\"year_month\"] = pd.to_datetime(properties_with_tract[\"assessment_date\"]).dt.to_period(\"M\")\n\naggregated_data = properties_with_tract.groupby([\"GEOID\", \"year_month\"]).agg(\n    property_count=(\"cartodb_id\", \"count\"),\n    avg_sale_price=(\"sale_price\", \"mean\")\n).reset_index()\n\naggregated_data = aggregated_data.merge(\n    census_gdf[[\"GEOID\", \"geometry\"]], on=\"GEOID\", how=\"left\"\n)\naggregated_gdf = gpd.GeoDataFrame(aggregated_data, geometry=\"geometry\", crs=\"EPSG:3857\")\n\nC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_24168\\1386209769.py:6: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n  properties_with_tract[\"year_month\"] = pd.to_datetime(properties_with_tract[\"assessment_date\"]).dt.to_period(\"M\")\n\n\n\n#and we can see, for same tract, not every month will have property register record\naggregated_gdf.tail()\n\n\n\n\n\n\n\n\nGEOID\nyear_month\nproperty_count\navg_sale_price\ngeometry\n\n\n\n\n2386\n42101980900\n2024-06\n338\n2.272857e+06\nPOLYGON ((-8378024.880 4848026.880, -8377966.4...\n\n\n2387\n42101980900\n2024-09\n7\n9.213524e+06\nPOLYGON ((-8378024.880 4848026.880, -8377966.4...\n\n\n2388\n42101980900\n2024-11\n1\n5.655000e+05\nPOLYGON ((-8378024.880 4848026.880, -8377966.4...\n\n\n2389\n42101989100\n2023-05\n4\n1.250000e+00\nPOLYGON ((-8351467.390 4870423.710, -8351384.2...\n\n\n2390\n42101989100\n2024-06\n45\n7.283044e+05\nPOLYGON ((-8351467.390 4870423.710, -8351384.2...\n\n\n\n\n\n\n\n\nexample_month = \"2024-06\"\nplot_data = aggregated_gdf[aggregated_gdf[\"year_month\"] == example_month]\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\ncensus_gdf.plot(ax=ax, color=\"lightgrey\", edgecolor=\"black\")\nplot_data.plot(ax=ax, column=\"property_count\", cmap=\"OrRd\", legend=True)\nax.set_title(f\"Housing Data Aggregation - {example_month}\")\nplt.show()\n\n\n\n\n\n\n\n\n\n#now we will investigate the pattern of house prices and their character by regions\n\n\nimport geopandas as gpd\nfrom folium import GeoJson\n\nrelative_path = \"final/Philadelphia_ACS_2023.shp\"\n\ntry:\n    census_gdf = gpd.read_file(relative_path)\nexcept FileNotFoundError:\n    raise FileNotFoundError(f\"The file at {relative_path} does not exist. Please check the path.\")\n\nif census_gdf.crs != \"EPSG:3857\":\n    census_gdf = census_gdf.to_crs(epsg=3857)\n\nprint(f\"Census Tract data loaded with {len(census_gdf)} rows.\")\n\nCensus Tract data loaded with 384 rows.\n\n\n\nproperties_with_tract = gpd.sjoin(\n    cleaned_properties_gdf, census_gdf, how=\"inner\", predicate=\"intersects\"\n)\n\nproperties_with_tract[\"year_month\"] = pd.to_datetime(properties_with_tract[\"assessment_date\"]).dt.to_period(\"M\")\n\naggregated_data = properties_with_tract.groupby([\"GEOID\", \"year_month\"]).agg(\n    property_count=(\"cartodb_id\", \"count\"),\n    avg_sale_price=(\"sale_price\", \"mean\")\n).reset_index()\n\naggregated_data['GEOID'] = aggregated_data['GEOID'].astype(str)\ncensus_gdf['GEOID'] = census_gdf['GEOID'].astype(str)\n\ncommon_geoids = set(aggregated_data['GEOID']).intersection(set(census_gdf['GEOID']))\n\nfiltered_census_gdf = census_gdf[census_gdf['GEOID'].isin(common_geoids)]\nfiltered_aggregated_gdf = aggregated_data[aggregated_data['GEOID'].isin(common_geoids)]\n\nfinal_aggregated_gdf = filtered_aggregated_gdf.merge(\n    filtered_census_gdf[['GEOID', 'geometry']], \n    on=\"GEOID\", \n    how=\"left\"\n)\n\nfinal_aggregated_gdf = gpd.GeoDataFrame(final_aggregated_gdf, geometry=\"geometry\", crs=\"EPSG:3857\")\n\nprint(f\"Final Aggregated Data Count: {len(final_aggregated_gdf)}\")\n\nFinal Aggregated Data Count: 2391\n\n\nC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_24168\\1647549544.py:5: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n  properties_with_tract[\"year_month\"] = pd.to_datetime(properties_with_tract[\"assessment_date\"]).dt.to_period(\"M\")\n\n\n\nfinal_aggregated_gdf.tail()\n\n\n\n\n\n\n\n\nGEOID\nyear_month\nproperty_count\navg_sale_price\ngeometry\n\n\n\n\n2386\n42101980900\n2024-06\n338\n2.272857e+06\nPOLYGON ((-8378024.880 4848026.880, -8377966.4...\n\n\n2387\n42101980900\n2024-09\n7\n9.213524e+06\nPOLYGON ((-8378024.880 4848026.880, -8377966.4...\n\n\n2388\n42101980900\n2024-11\n1\n5.655000e+05\nPOLYGON ((-8378024.880 4848026.880, -8377966.4...\n\n\n2389\n42101989100\n2023-05\n4\n1.250000e+00\nPOLYGON ((-8351467.390 4870423.710, -8351384.2...\n\n\n2390\n42101989100\n2024-06\n45\n7.283044e+05\nPOLYGON ((-8351467.390 4870423.710, -8351384.2...\n\n\n\n\n\n\n\n\n#here, I try to join social and housing data together\nproperties_with_tract = gpd.sjoin(\n    cleaned_properties_gdf, census_gdf, how=\"inner\", predicate=\"intersects\"\n)\n\nbuffered_census_gdf = census_gdf.copy()\nbuffered_census_gdf[\"geometry\"] = buffered_census_gdf.geometry.buffer(10)\n\nproperties_with_tract = gpd.sjoin(\n    cleaned_properties_gdf, buffered_census_gdf, how=\"inner\", predicate=\"intersects\"\n)\n\nproperties_with_tract = gpd.GeoDataFrame(\n    properties_with_tract,\n    geometry='parsed_geometry',\n    crs=cleaned_properties_gdf.crs\n)\n\nprint(len(properties_with_tract))\n\nproperties_with_tract[\"assessment_date\"] = pd.to_datetime(properties_with_tract[\"assessment_date\"], errors=\"coerce\")\nproperties_with_tract[\"year_month\"] = properties_with_tract[\"assessment_date\"].dt.to_period(\"M\")\n\n583942\n\n\nC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_24168\\2193257280.py:22: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n  properties_with_tract[\"year_month\"] = properties_with_tract[\"assessment_date\"].dt.to_period(\"M\")\n\n\n\nif properties_with_tract[\"year_month\"].isnull().sum() &gt; 0:\n    properties_with_tract[\"year_month\"] = properties_with_tract[\"year_month\"].fillna(\"Unknown\")\n\nproperties_with_tract.tail()\n\n#now datas are toegther!\n\n\n\n\n\n\n\n\ncartodb_id\nthe_geom\nthe_geom_webmercator\nassessment_date\nbasements\ncensus_tract\ncentral_air\nfireplaces\nfrontage\nfuel\n...\nTotal Ho_1\nOccupied H\nVacant Hou\nOwner-occu\nRenter-occ\nNAME\nstate\ncounty\ntract\nyear_month\n\n\n\n\n583657\n584047\n0101000020E6100000CA62F43784C452C0553088271300...\n0101000020110F0000DF8AED67F4E05FC1CB865FCEFA8F...\n2024-06-06 16:07:08+00:00\nD\n183\nN\n0.0\n20.0\nNone\n...\n1753.0\n1539.0\n214.0\n1344.0\n195.0\nCensus Tract 183, Philadelphia County, Pennsyl...\n42\n101\n018300\n2024-06\n\n\n583658\n584048\n0101000020E61000002876388229C352C0DAF8E3121703...\n0101000020110F00002E29FC7BA7DE5FC15D1760C65293...\n2024-06-06 16:05:14+00:00\nNone\n323\nN\n0.0\n72.0\nNone\n...\n1611.0\n1419.0\n192.0\n787.0\n632.0\nCensus Tract 323, Philadelphia County, Pennsyl...\n42\n101\n032300\n2024-06\n\n\n583659\n584049\n0101000020E61000009638723466CA52C07637567AF002...\n0101000020110F0000C30DA979F2EA5FC11BC631F82793...\n2024-11-16 12:29:40+00:00\nNone\n244\nNone\nNaN\nNaN\nNone\n...\n1301.0\n1011.0\n290.0\n460.0\n551.0\nCensus Tract 244, Philadelphia County, Pennsyl...\n42\n101\n024400\n2024-11\n\n\n583660\n584050\n0101000020E61000009638723466CA52C07637567AF002...\n0101000020110F0000C30DA979F2EA5FC11BC631F82793...\n2024-12-14 16:53:16+00:00\nNone\n244\nNone\nNaN\nNaN\nNone\n...\n1301.0\n1011.0\n290.0\n460.0\n551.0\nCensus Tract 244, Philadelphia County, Pennsyl...\n42\n101\n024400\n2024-12\n\n\n583661\n584051\n0101000020E6100000C75E7D144DC552C04F40CC0FA801...\n0101000020110F0000F891E19649E25FC1DADDADC3BB91...\n2024-06-06 16:05:55+00:00\nNone\n294\nNone\nNaN\n30.0\nNone\n...\n1337.0\n1127.0\n210.0\n496.0\n631.0\nCensus Tract 294, Philadelphia County, Pennsyl...\n42\n101\n029400\n2024-06\n\n\n\n\n5 rows × 58 columns\n\n\n\n\nproperties_with_tract = properties_with_tract.fillna(0)\n\nC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_24168\\2867799837.py:1: DeprecationWarning: ExtensionArray.fillna added a 'copy' keyword in pandas 2.1.0. In a future version, ExtensionArray subclasses will need to implement this keyword or an exception will be raised. In the interim, the keyword is ignored by GeometryArray.\n  properties_with_tract = properties_with_tract.fillna(0)\n\n\n\nfrom shapely.wkb import loads as wkb_loads \nif \"geometry\" not in properties_with_tract.columns:\n    if \"the_geom\" in properties_with_tract.columns:\n        print(\"Converting 'the_geom' to 'geometry'...\")\n        properties_with_tract = gpd.GeoDataFrame(\n            properties_with_tract,\n            geometry=properties_with_tract[\"the_geom\"].apply(wkb_loads),\n            crs=\"EPSG:4326\" \n        )\n    elif \"the_geom_webmercator\" in properties_with_tract.columns:\n        print(\"Converting 'the_geom_webmercator' to 'geometry'...\")\n        properties_with_tract = gpd.GeoDataFrame(\n            properties_with_tract,\n            geometry=properties_with_tract[\"the_geom_webmercator\"].apply(wkb_loads),\n            crs=\"EPSG:3857\" \n        )\n    else:\n        raise ValueError(\"No valid geometry column ('geometry', 'the_geom', or 'the_geom_webmercator') found!\")\n\nConverting 'the_geom' to 'geometry'...\n\n\n\nprint(final_aggregated_gdf.head())\nprint(final_aggregated_gdf.crs)\n\n         GEOID year_month  property_count  avg_sale_price  \\\n0  42101000100    2023-05              59    3.703884e+06   \n1  42101000100    2023-08               7    4.441428e+06   \n2  42101000100    2023-09               2    5.725000e+06   \n3  42101000100    2024-02               3    2.355000e+06   \n4  42101000100    2024-03               1    2.025000e+06   \n\n                                            geometry  \n0  POLYGON ((-8365905.970 4858674.430, -8365885.3...  \n1  POLYGON ((-8365905.970 4858674.430, -8365885.3...  \n2  POLYGON ((-8365905.970 4858674.430, -8365885.3...  \n3  POLYGON ((-8365905.970 4858674.430, -8365885.3...  \n4  POLYGON ((-8365905.970 4858674.430, -8365885.3...  \nEPSG:3857\n\n\n\n##merely test!!!! I got confused at this point\nprint(\"Grouping housing data by GEOID...\")\naggregated_df = properties_with_tract.groupby(\"GEOID\", as_index=False).agg({\n    \"sale_price\": \"mean\",\n    \"cartodb_id\": \"count\",\n})\n\naggregated_df.rename(\n    columns={\n        \"sale_price\": \"avg_sale_price\",\n        \"cartodb_id\": \"property_count\"\n    },\n    inplace=True\n)\n\n# merge back\ncensus_gdf[\"GEOID\"] = census_gdf[\"GEOID\"].astype(str)\naggregated_df[\"GEOID\"] = aggregated_df[\"GEOID\"].astype(str)\n\nfinal_aggregated_data = census_gdf.merge(\n    aggregated_df,\n    on=\"GEOID\",\n    how=\"left\"  # all tracts kept\n)\n\n# all data here!\nprint(\"Final aggregated data shape:\", final_aggregated_data.shape)\n\nprint(\"Final GeoDataFrame created successfully!\")\nprint(final_aggregated_data.head())\nprint(\"Geometry type:\", final_aggregated_data.geometry.geom_type.value_counts())\n\nGrouping housing data by GEOID...\nFinal aggregated data shape: (384, 25)\nFinal GeoDataFrame created successfully!\n         GEOID  Total Popu  Median Age  White Alon  Black or A  Asian Alon  \\\n0  42101009802      6190.0        33.5       580.0      5341.0        35.0   \n1  42101037500      3736.0        31.3      1672.0      1467.0       165.0   \n2  42101021900      1434.0        44.7      1307.0        50.0        24.0   \n3  42101011300      3344.0        28.5        62.0      3229.0         0.0   \n4  42101021800      5141.0        30.9      2822.0      1497.0       247.0   \n\n   Hispanic o  Total Hous  Total Fami  Married-co  ...  Vacant Hou  \\\n0       149.0      2129.0      1467.0       576.0  ...       242.0   \n1       298.0      1224.0       733.0       547.0  ...       159.0   \n2        41.0       654.0       307.0       238.0  ...        97.0   \n3        53.0      1102.0       770.0       133.0  ...       290.0   \n4       178.0      2242.0      1073.0       887.0  ...       152.0   \n\n   Owner-occu  Renter-occ                                               NAME  \\\n0      1593.0       536.0  Census Tract 98.02, Philadelphia County, Penns...   \n1       722.0       502.0  Census Tract 375, Philadelphia County, Pennsyl...   \n2       551.0       103.0  Census Tract 219, Philadelphia County, Pennsyl...   \n3       621.0       481.0  Census Tract 113, Philadelphia County, Pennsyl...   \n4       597.0      1645.0  Census Tract 218, Philadelphia County, Pennsyl...   \n\n   state  county   tract                                           geometry  \\\n0     42     101  009802  POLYGON ((-8379626.770 4862662.870, -8379606.2...   \n1     42     101  037500  POLYGON ((-8378518.360 4863306.140, -8378433.4...   \n2     42     101  021900  POLYGON ((-8377278.820 4872712.160, -8377218.9...   \n3     42     101  011300  POLYGON ((-8375619.820 4863198.200, -8375613.5...   \n4     42     101  021800  POLYGON ((-8375464.090 4874647.550, -8375257.5...   \n\n  avg_sale_price property_count  \n0  154038.260785           2063  \n1  407074.872364            901  \n2  171537.744552            826  \n3   56054.240363           1323  \n4  195453.591292            712  \n\n[5 rows x 25 columns]\nGeometry type: Polygon    384\nName: count, dtype: int64\n\n\n\n\nprint(\"Grouping geometry data by GEOID...\")\ngeometry_data = properties_with_tract.groupby(\"GEOID\").geometry.first()\n\n\nfinal_aggregated_data = gpd.GeoDataFrame(\n    final_aggregated_data,\n    geometry=geometry_data,\n    crs=properties_with_tract.crs\n)\n\nprint(\"Final GeoDataFrame created successfully!\")\n\nprint(\"Sample geometries:\")\nprint(properties_with_tract.geometry.head())\nprint(\"Geometry type:\")\nprint(properties_with_tract.geometry.geom_type.value_counts())\n\nGrouping geometry data by GEOID...\nFinal GeoDataFrame created successfully!\nSample geometries:\n0    POINT (-75.18187 39.99381)\n1    POINT (-75.18190 39.98864)\n2    POINT (-75.24368 39.92280)\n3    POINT (-75.17936 39.98911)\n4    POINT (-75.16592 39.98405)\nName: geometry, dtype: geometry\nGeometry type:\nPoint    583942\nName: count, dtype: int64\n\n\n\ngeometry_data = geometry_data.reindex(final_aggregated_data[\"GEOID\"]).reset_index(drop=True)\n\nfinal_aggregated_data = gpd.GeoDataFrame(\n    final_aggregated_data,\n    geometry=geometry_data,\n    crs=properties_with_tract.crs\n)\n\n\n#now lets make a merge to census tract data from different month, and find out price and feature, static data are fixed while I want to see changes across month for dynamic data\n\n\nfrom shapely.wkb import loads as wkb_loads\n\nif \"geometry\" not in properties_with_tract.columns:\n    if \"the_geom\" in properties_with_tract.columns:\n        properties_with_tract = gpd.GeoDataFrame(\n            properties_with_tract,\n            geometry=properties_with_tract[\"the_geom\"].apply(wkb_loads),\n            crs=\"EPSG:3857\"\n        )\n    elif \"the_geom_webmercator\" in properties_with_tract.columns:\n        properties_with_tract = gpd.GeoDataFrame(\n            properties_with_tract,\n            geometry=properties_with_tract[\"the_geom_webmercator\"].apply(wkb_loads),\n            crs=\"EPSG:3857\"\n        )\n    else:\n        raise ValueError(\"No valid geometry column found!\")\n\ngeometry_data = properties_with_tract.groupby(\"GEOID\").geometry.first()\nprint(\"Geometry data after grouping:\")\nprint(geometry_data.head())\nprint(\"Null geometries count:\", geometry_data.isnull().sum())\n\ngeometry_data = geometry_data.reindex(final_aggregated_data[\"GEOID\"]).reset_index(drop=True)\n\nfinal_aggregated_data = gpd.GeoDataFrame(\n    final_aggregated_data,\n    geometry=geometry_data,\n    crs=properties_with_tract.crs\n)\n\nprint(\"Final aggregated data geometries:\")\nprint(final_aggregated_data.geometry.head())\nprint(\"Geometry types:\")\nprint(final_aggregated_data.geometry.geom_type.value_counts())\n\nGeometry data after grouping:\nGEOID\n42101000100    POINT (-75.14808 39.95194)\n42101000200    POINT (-75.16098 39.95747)\n42101000300    POINT (-75.16542 39.95504)\n42101000401    POINT (-75.17577 39.95263)\n42101000402    POINT (-75.16494 39.95148)\nName: geometry, dtype: geometry\nNull geometries count: 0\nFinal aggregated data geometries:\n0    POINT (-75.26348 39.97478)\n1    POINT (-75.25241 39.98796)\n2    POINT (-75.24063 40.04991)\n3    POINT (-75.23341 39.97987)\n4    POINT (-75.23413 40.05904)\nName: geometry, dtype: geometry\nGeometry types:\nPoint    384\nName: count, dtype: int64\n\n\n\nproperties_with_tract[\"property_count\"] = 1\n\naggregated_data = (\n    properties_with_tract\n    .groupby(\"GEOID\", as_index=False) # no month any more, I previously did by month, removed now\n    .agg(\n        total_property_count=(\"property_count\", \"sum\"),\n        avg_rooms=(\"number_of_rooms\", \"mean\"),\n        avg_bathrooms=(\"number_of_bathrooms\", \"mean\"),\n        avg_bedrooms=(\"number_of_bedrooms\", \"mean\"),\n        avg_sale_price=(\"sale_price\", \"mean\"),\n        avg_market_value=(\"market_value\", \"mean\")\n    )\n)\naggregated_data[\"avg_sale_price\"] = (aggregated_data[\"avg_sale_price\"] / 1_000_000).round(2)\naggregated_data[\"avg_market_value\"] = (aggregated_data[\"avg_market_value\"] / 1_000_000).round(2)\n\nstatic_columns = [\n    \"GEOID\", \"Total Popu\", \"Median Age\", \"White Alon\", \"Black or A\", \n    \"Asian Alon\", \"Hispanic o\", \"Total Hous\", \"Total Fami\", \n    \"Married-co\", \"Poverty St\", \"Below Pove\", \"Median Hou\", \n    \"Occupied H\", \"Vacant Hou\", \"Owner-occu\", \"Renter-occ\"\n]\n\nstatic_data = (\n    properties_with_tract[static_columns]\n    .drop_duplicates(subset=\"GEOID\")\n)\n\nfinal_aggregated_data = aggregated_data.merge(\n    static_data, on=\"GEOID\", how=\"left\"\n)\n\ncensus_gdf[\"GEOID\"] = census_gdf[\"GEOID\"].astype(str)\nfinal_aggregated_data[\"GEOID\"] = final_aggregated_data[\"GEOID\"].astype(str)\n\nfinal_aggregated_data = census_gdf.merge(\n    final_aggregated_data,\n    on=\"GEOID\",\n    how=\"left\"\n)\n\nfinal_aggregated_data = gpd.GeoDataFrame(\n    final_aggregated_data,\n    geometry=final_aggregated_data.geometry,\n    crs=census_gdf.crs\n)\n\nprint(\"Final aggregated data geometry type:\")\nprint(final_aggregated_data.geometry.geom_type.value_counts())\nprint(final_aggregated_data.columns)\n#here I encountered some duplicated column names, so I drop them later, u may ask why I not going back? I dont really know what is the source of the problem, so maybe just not touch it\n\nFinal aggregated data geometry type:\nPolygon    384\nName: count, dtype: int64\nIndex(['GEOID', 'Total Popu_x', 'Median Age_x', 'White Alon_x', 'Black or A_x',\n       'Asian Alon_x', 'Hispanic o_x', 'Total Hous_x', 'Total Fami_x',\n       'Married-co_x', 'Poverty St_x', 'Below Pove_x', 'Median Hou_x',\n       'Total Ho_1', 'Occupied H_x', 'Vacant Hou_x', 'Owner-occu_x',\n       'Renter-occ_x', 'NAME', 'state', 'county', 'tract', 'geometry',\n       'total_property_count', 'avg_rooms', 'avg_bathrooms', 'avg_bedrooms',\n       'avg_sale_price', 'avg_market_value', 'Total Popu_y', 'Median Age_y',\n       'White Alon_y', 'Black or A_y', 'Asian Alon_y', 'Hispanic o_y',\n       'Total Hous_y', 'Total Fami_y', 'Married-co_y', 'Poverty St_y',\n       'Below Pove_y', 'Median Hou_y', 'Occupied H_y', 'Vacant Hou_y',\n       'Owner-occu_y', 'Renter-occ_y'],\n      dtype='object')\n\n\n\ncols_to_drop = [col for col in final_aggregated_data.columns if col.endswith('_x')]\nfinal_aggregated_data.drop(columns=cols_to_drop, inplace=True)\nprint(final_aggregated_data.columns)\n\nIndex(['GEOID', 'Total Ho_1', 'NAME', 'state', 'county', 'tract', 'geometry',\n       'total_property_count', 'avg_rooms', 'avg_bathrooms', 'avg_bedrooms',\n       'avg_sale_price', 'avg_market_value', 'Total Popu_y', 'Median Age_y',\n       'White Alon_y', 'Black or A_y', 'Asian Alon_y', 'Hispanic o_y',\n       'Total Hous_y', 'Total Fami_y', 'Married-co_y', 'Poverty St_y',\n       'Below Pove_y', 'Median Hou_y', 'Occupied H_y', 'Vacant Hou_y',\n       'Owner-occu_y', 'Renter-occ_y'],\n      dtype='object')\n\n\n\nprint(final_aggregated_data.tail())\n\n           GEOID  Total Ho_1  \\\n379  42101035602      1284.0   \n380  42101035601      2404.0   \n381  42101033102      1760.0   \n382  42101034801      2006.0   \n383  42101036201      2153.0   \n\n                                                  NAME state county   tract  \\\n379  Census Tract 356.02, Philadelphia County, Penn...    42    101  035602   \n380  Census Tract 356.01, Philadelphia County, Penn...    42    101  035601   \n381  Census Tract 331.02, Philadelphia County, Penn...    42    101  033102   \n382  Census Tract 348.01, Philadelphia County, Penn...    42    101  034801   \n383  Census Tract 362.01, Philadelphia County, Penn...    42    101  036201   \n\n                                              geometry  total_property_count  \\\n379  POLYGON ((-8355008.800 4881749.700, -8354858.9...                  1126   \n380  POLYGON ((-8354256.940 4879308.850, -8354227.1...                  2064   \n381  POLYGON ((-8352727.080 4872434.270, -8352711.8...                  1410   \n382  POLYGON ((-8351724.540 4874706.750, -8351706.9...                   875   \n383  POLYGON ((-8348332.630 4877819.050, -8348033.1...                  1563   \n\n     avg_rooms  avg_bathrooms  ...  Total Hous_y  Total Fami_y  Married-co_y  \\\n379   0.000000       0.267318  ...        1252.0         999.0         834.0   \n380   0.008721       0.706880  ...        2155.0        1344.0         964.0   \n381   0.111348       0.735461  ...        1556.0         970.0         651.0   \n382   0.272000       0.657143  ...        1969.0        1215.0         761.0   \n383   0.000000       0.852847  ...        1981.0        1257.0         839.0   \n\n     Poverty St_y  Below Pove_y  Median Hou_y  Occupied H_y  Vacant Hou_y  \\\n379         999.0           0.0       81912.0        1252.0          32.0   \n380        1344.0         172.0       52673.0        2155.0         249.0   \n381         970.0         119.0       42692.0        1556.0         204.0   \n382        1215.0          34.0       57463.0        1969.0          37.0   \n383        1257.0          57.0       74360.0        1981.0         172.0   \n\n     Owner-occu_y  Renter-occ_y  \n379        1022.0         230.0  \n380        1398.0         757.0  \n381         935.0         621.0  \n382         922.0        1047.0  \n383        1327.0         654.0  \n\n[5 rows x 29 columns]\n\n\n\ndef fix_missing_rooms(gdf):\n    mask_missing_rooms = gdf[\"avg_rooms\"].isna()\n    mask_have_bed_bath = (~gdf[\"avg_bedrooms\"].isna()) & (~gdf[\"avg_bathrooms\"].isna())\n\n    gdf.loc[mask_missing_rooms & mask_have_bed_bath, \"avg_rooms\"] = (\n        gdf[\"avg_bedrooms\"] + gdf[\"avg_bathrooms\"] + 0.5\n    )\n    gdf.loc[mask_missing_rooms & ~mask_have_bed_bath, \"avg_rooms\"] = 1\n\n    return gdf\n\nfinal_aggregated_data = fix_missing_rooms(final_aggregated_data)\n\n\n#now create panel selector\ndynamic_cols = [\n    'total_property_count', 'avg_rooms', 'avg_bathrooms',\n    'avg_bedrooms', 'avg_sale_price', 'avg_market_value'\n]\n\n#also let guys see the sale and market value\ntooltip_cols = [\n    'Total Popu_y', 'Median Age_y','White Alon_y', 'Black or A_y', 'Asian Alon_y',\n    'Hispanic o_y','Total Hous_y', 'Total Fami_y', 'Married-co_y','Poverty St_y',\n    'Below Pove_y', 'Median Hou_y', 'Occupied H_y', 'Vacant Hou_y',\n    'Owner-occu_y', 'Renter-occ_y'\n]\n\nfinal_aggregated_data = final_aggregated_data.to_crs(epsg=4326)\n\nfrom shapely.geometry import mapping\n\n\nimport folium\nimport geopandas as gpd\nimport panel as pn\nimport altair as alt\nimport mapclassify\nimport numpy as np\n\ngdf = final_aggregated_data\n\n###############################\n# Function/Legend!\n###############################\ndef get_natural_breaks_bins(data_series, k=8, lower_q=0.01, upper_q=0.99):\n    series_no_na = data_series.dropna()\n    lower_val = series_no_na.quantile(lower_q)\n    upper_val = series_no_na.quantile(upper_q)\n    clipped_vals = np.clip(series_no_na, lower_val, upper_val)\n\n    nb = mapclassify.NaturalBreaks(clipped_vals, k=k)\n    breaks = nb.bins.tolist()\n\n    min_val = series_no_na.min()\n    max_val = series_no_na.max()\n    bins = [min_val] + breaks\n    bins[-1] = max_val\n    return bins\n\ndef create_legend_html(bins, legend_name):\n    labels = []\n    for i in range(len(bins)-1):\n        start_val = bins[i]\n        end_val = bins[i+1]\n        labels.append(f\"{start_val:.2f} ~ {end_val:.2f}\")\n\n    legend_items = \"\".join([f\"&lt;li&gt;{lbl}&lt;/li&gt;\" for lbl in labels])\n    legend_html = f\"\"\"\n    &lt;div style=\"\n        position: absolute; \n        z-index:9999; \n        bottom: 80px; \n        right: 10px; \n        background-color: white;\n        border:2px solid grey;\n        border-radius:5px;\n        padding: 10px;\n    \"&gt;\n      &lt;h4 style=\"margin-top:0\"&gt;{legend_name}&lt;/h4&gt;\n      &lt;ul style=\"list-style:none; padding-left:0; margin:0;\"&gt;\n        {legend_items}\n      &lt;/ul&gt;\n    &lt;/div&gt;\n    \"\"\"\n    return legend_html\n\n###############################\n# Map!\n###############################\ndef create_folium_map(selected_col):\n    center_lat = gdf.geometry.centroid.y.mean()\n    center_lon = gdf.geometry.centroid.x.mean()\n\n    m = folium.Map(\n        location=[center_lat, center_lon],\n        zoom_start=11,\n        tiles='CartoDB Positron',\n        control_scale=True\n    )\n\n    geojson_data = gdf.to_json()\n\n    bins = get_natural_breaks_bins(gdf[selected_col], k=8, lower_q=0.01, upper_q=0.99)\n\n    choropleth = folium.Choropleth(\n        geo_data=geojson_data,\n        data=gdf,\n        columns=['GEOID', selected_col],\n        key_on='feature.properties.GEOID',\n        bins=bins,\n        fill_color='YlOrRd',\n        fill_opacity=0.7,\n        line_opacity=0.2,\n        highlight=True,\n        legend_name=f'{selected_col} (Natural Breaks)',\n    )\n    choropleth.add_to(m)\n\n    style_statement = \"\"\"\n    &lt;style&gt;\n    .legend {\n      display: none !important;\n    }\n    &lt;/style&gt;\n    \"\"\"\n    m.get_root().header.add_child(folium.Element(style_statement))\n\n    # Tooltip\n    folium.GeoJsonTooltip(\n        fields=['GEOID'] + tooltip_cols,\n        aliases=['GEOID'] + tooltip_cols,\n        localize=True\n    ).add_to(choropleth.geojson)\n\n    legend_html = create_legend_html(bins, f\"{selected_col} (Natural Breaks)\")\n    m.get_root().html.add_child(folium.Element(legend_html))\n\n    return m\n\n###############################\n# Histo!\n###############################\ndef create_histogram(selected_col):\n    df = gdf.copy()\n    chart = (\n        alt.Chart(df)\n        .mark_bar()\n        .encode(\n            x=alt.X(f\"{selected_col}:Q\", bin=alt.Bin(maxbins=30), title=f\"{selected_col}\"),\n            y=alt.Y('count()', title='Count')\n        )\n        .properties(\n            width=400,\n            height=250,\n            title=f\"Distribution of {selected_col} (Natural Breaks used on map)\"\n        )\n    )\n    return chart\n\n###############################\n# 4. Panel\n###############################\nselect_widget = pn.widgets.Select(\n    name=\"Select a column to visualize\",\n    options=dynamic_cols,\n    value=dynamic_cols[0]\n)\n\n@pn.depends(select_widget.param.value)\ndef update_view(selected_col):\n    folium_map = create_folium_map(selected_col)\n    folium_html = folium_map._repr_html_()\n    folium_pane = pn.pane.HTML(folium_html, width=700, height=500)\n\n    hist_chart = create_histogram(selected_col)\n    hist_pane = pn.pane.Vega(hist_chart, width=400, height=300)\n    return pn.Row(folium_pane, hist_pane)\n\ndashboard = pn.Column(\n    \"# Census Tract Interactive Dashboard\",\n    \"select indicator to continue：\",\n    select_widget,\n    update_view\n)\n\ndashboard.servable()\n\nC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_24168\\4061076871.py:59: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  center_lat = gdf.geometry.centroid.y.mean()\nC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_24168\\4061076871.py:60: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  center_lon = gdf.geometry.centroid.x.mean()\nC:\\Users\\44792\\miniforge3\\envs\\musa-550-fall-2023\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n  warnings.warn(\n\n\n\n\n\n\n  \n\n\n\n\n\n###Now, Question 1, OSL \n\n\ncrime_gdf = crime_gdf.to_crs(epsg=3857)\n\ncensus_gdf = census_gdf.to_crs(epsg=3857)\n\ncrime_by_tract = gpd.sjoin(\n    crime_gdf,     \n    census_gdf,   \n    how='inner', \n    predicate='intersects'\n)\n\nprint(\"crime_by_tract columns:\", crime_by_tract.columns)\nprint(\"Number of records after sjoin:\", len(crime_by_tract))\n\ncrime_by_tract columns: Index(['cartodb_id', 'the_geom', 'the_geom_webmercator', 'objectid', 'dc_dist',\n       'psa', 'dispatch_date_time', 'dispatch_date', 'dispatch_time', 'hour',\n       'dc_key', 'location_block', 'ucr_general', 'text_general_code',\n       'point_x', 'point_y', 'geometry', 'year_month', 'crime_category',\n       'index_right', 'GEOID', 'Total Popu', 'Median Age', 'White Alon',\n       'Black or A', 'Asian Alon', 'Hispanic o', 'Total Hous', 'Total Fami',\n       'Married-co', 'Poverty St', 'Below Pove', 'Median Hou', 'Total Ho_1',\n       'Occupied H', 'Vacant Hou', 'Owner-occu', 'Renter-occ', 'NAME', 'state',\n       'county', 'tract'],\n      dtype='object')\nNumber of records after sjoin: 462145\n\n\n\ncrime_count_by_tract = crime_by_tract.groupby(\"GEOID\").size().reset_index(name=\"total_crime_count\")\nprint(crime_count_by_tract.head())\ncrime_count_by_tract_type = crime_by_tract.groupby([\"GEOID\",\"crime_category\"]).size().unstack(fill_value=0)\n\n         GEOID  total_crime_count\n0  42101000100               2047\n1  42101000200               1192\n2  42101000300               1946\n3  42101000401                810\n4  42101000402               2991\n\n\n\ncensus_gdf = census_gdf.merge(crime_count_by_tract, on=\"GEOID\", how=\"left\")\n\ncensus_gdf[\"total_crime_count\"] = census_gdf[\"total_crime_count\"].fillna(0)\n\ncensus_gdf.head()\n\n\n\n\n\n\n\n\nGEOID\nTotal Popu\nMedian Age\nWhite Alon\nBlack or A\nAsian Alon\nHispanic o\nTotal Hous\nTotal Fami\nMarried-co\n...\nOccupied H\nVacant Hou\nOwner-occu\nRenter-occ\nNAME\nstate\ncounty\ntract\ngeometry\ntotal_crime_count\n\n\n\n\n0\n42101009802\n6190.0\n33.5\n580.0\n5341.0\n35.0\n149.0\n2129.0\n1467.0\n576.0\n...\n2129.0\n242.0\n1593.0\n536.0\nCensus Tract 98.02, Philadelphia County, Penns...\n42\n101\n009802\nPOLYGON ((-8379626.770 4862662.870, -8379606.2...\n1031\n\n\n1\n42101037500\n3736.0\n31.3\n1672.0\n1467.0\n165.0\n298.0\n1224.0\n733.0\n547.0\n...\n1224.0\n159.0\n722.0\n502.0\nCensus Tract 375, Philadelphia County, Pennsyl...\n42\n101\n037500\nPOLYGON ((-8378518.360 4863306.140, -8378433.4...\n486\n\n\n2\n42101021900\n1434.0\n44.7\n1307.0\n50.0\n24.0\n41.0\n654.0\n307.0\n238.0\n...\n654.0\n97.0\n551.0\n103.0\nCensus Tract 219, Philadelphia County, Pennsyl...\n42\n101\n021900\nPOLYGON ((-8377278.820 4872712.160, -8377218.9...\n167\n\n\n3\n42101011300\n3344.0\n28.5\n62.0\n3229.0\n0.0\n53.0\n1102.0\n770.0\n133.0\n...\n1102.0\n290.0\n621.0\n481.0\nCensus Tract 113, Philadelphia County, Pennsyl...\n42\n101\n011300\nPOLYGON ((-8375619.820 4863198.200, -8375613.5...\n975\n\n\n4\n42101021800\n5141.0\n30.9\n2822.0\n1497.0\n247.0\n178.0\n2242.0\n1073.0\n887.0\n...\n2242.0\n152.0\n597.0\n1645.0\nCensus Tract 218, Philadelphia County, Pennsyl...\n42\n101\n021800\nPOLYGON ((-8375464.090 4874647.550, -8375257.5...\n490\n\n\n\n\n5 rows × 24 columns\n\n\n\n\ncensus_gdf[\"white_pop\"] = census_gdf[\"White Alon\"]\ncensus_gdf[\"black_pop\"] = census_gdf[\"Black or A\"]\ncensus_gdf[\"hisp_pop\"] = census_gdf[\"Hispanic o\"]\n\ncensus_gdf[\"total_pop\"] = census_gdf[\"Total Popu\"]\n\ncensus_gdf[\"other_pop\"] = census_gdf[\"total_pop\"] - (\n    census_gdf[\"white_pop\"] + census_gdf[\"black_pop\"] + census_gdf[\"hisp_pop\"]\n)\n\n# clip out negative\ncensus_gdf[\"other_pop\"] = census_gdf[\"other_pop\"].clip(lower=0)\n\n# race ratio\ncensus_gdf[\"white_ratio\"] = census_gdf[\"white_pop\"] / census_gdf[\"total_pop\"]\ncensus_gdf[\"black_ratio\"] = census_gdf[\"black_pop\"] / census_gdf[\"total_pop\"]\ncensus_gdf[\"hisp_ratio\"]  = census_gdf[\"hisp_pop\"]  / census_gdf[\"total_pop\"]\ncensus_gdf[\"other_ratio\"] = census_gdf[\"other_pop\"] / census_gdf[\"total_pop\"]\n\ndef calc_diversity(row):\n    return 1 - (\n        (row[\"white_ratio\"]**2) +\n        (row[\"black_ratio\"]**2) +\n        (row[\"hisp_ratio\"]**2) +\n        (row[\"other_ratio\"]**2)\n    )\n\ncensus_gdf[\"diversity_index\"] = census_gdf.apply(calc_diversity, axis=1)\n\ncensus_gdf.head()\n\n\n\n\n\n\n\n\nGEOID\nTotal Popu\nMedian Age\nWhite Alon\nBlack or A\nAsian Alon\nHispanic o\nTotal Hous\nTotal Fami\nMarried-co\n...\nwhite_pop\nblack_pop\nhisp_pop\ntotal_pop\nother_pop\nwhite_ratio\nblack_ratio\nhisp_ratio\nother_ratio\ndiversity_index\n\n\n\n\n0\n42101009802\n6190.0\n33.5\n580.0\n5341.0\n35.0\n149.0\n2129.0\n1467.0\n576.0\n...\n580.0\n5341.0\n149.0\n6190.0\n120.0\n0.093700\n0.862843\n0.024071\n0.019386\n0.245767\n\n\n1\n42101037500\n3736.0\n31.3\n1672.0\n1467.0\n165.0\n298.0\n1224.0\n733.0\n547.0\n...\n1672.0\n1467.0\n298.0\n3736.0\n299.0\n0.447537\n0.392666\n0.079764\n0.080032\n0.632756\n\n\n2\n42101021900\n1434.0\n44.7\n1307.0\n50.0\n24.0\n41.0\n654.0\n307.0\n238.0\n...\n1307.0\n50.0\n41.0\n1434.0\n36.0\n0.911437\n0.034868\n0.028591\n0.025105\n0.166620\n\n\n3\n42101011300\n3344.0\n28.5\n62.0\n3229.0\n0.0\n53.0\n1102.0\n770.0\n133.0\n...\n62.0\n3229.0\n53.0\n3344.0\n0.0\n0.018541\n0.965610\n0.015849\n0.000000\n0.067002\n\n\n4\n42101021800\n5141.0\n30.9\n2822.0\n1497.0\n247.0\n178.0\n2242.0\n1073.0\n887.0\n...\n2822.0\n1497.0\n178.0\n5141.0\n644.0\n0.548920\n0.291188\n0.034624\n0.125267\n0.597005\n\n\n\n\n5 rows × 34 columns\n\n\n\n\nhouse_cols = [\"GEOID\", \"avg_sale_price\", \"avg_rooms\", \"total_property_count\"]\ncensus_gdf = census_gdf.merge(final_aggregated_data[house_cols], on=\"GEOID\", how=\"left\")\n\ncensus_gdf[[\"avg_sale_price\",\"avg_rooms\",\"total_property_count\"]] = \\\n    census_gdf[[\"avg_sale_price\",\"avg_rooms\",\"total_property_count\"]].fillna(0)\n\n\ncensus_gdf.head()\nprint(census_gdf.columns)\n\nIndex(['GEOID', 'Total Popu', 'Median Age', 'White Alon', 'Black or A',\n       'Asian Alon', 'Hispanic o', 'Total Hous', 'Total Fami', 'Married-co',\n       'Poverty St', 'Below Pove', 'Median Hou', 'Total Ho_1', 'Occupied H',\n       'Vacant Hou', 'Owner-occu', 'Renter-occ', 'NAME', 'state', 'county',\n       'tract', 'geometry', 'total_crime_count', 'white_pop', 'black_pop',\n       'hisp_pop', 'total_pop', 'other_pop', 'white_ratio', 'black_ratio',\n       'hisp_ratio', 'other_ratio', 'diversity_index', 'avg_sale_price',\n       'avg_rooms', 'total_property_count'],\n      dtype='object')\n\n\n\n#now we keep only those columns we think useful and possibly related (tho not all of them may use later)\ncolumns_to_keep = [\n    \"GEOID\",'Total Popu', 'Median Age','Poverty St', 'Below Pove','Married-co',\n    \"white_ratio\", \"black_ratio\", \"hisp_ratio\", \"other_ratio\", \"diversity_index\",\n    \"total_crime_count\", \"avg_sale_price\", \"avg_rooms\", \"total_property_count\",\n    \"geometry\"\n]\n\nfinal_cols = [col for col in columns_to_keep if col in census_gdf.columns]\ncensus_gdf = census_gdf[final_cols]\ncensus_gdf.head()\n\n\n\n\n\n\n\n\nGEOID\nTotal Popu\nMedian Age\nPoverty St\nBelow Pove\nMarried-co\nwhite_ratio\nblack_ratio\nhisp_ratio\nother_ratio\ndiversity_index\ntotal_crime_count\navg_sale_price\navg_rooms\ntotal_property_count\ngeometry\n\n\n\n\n0\n42101009802\n6190.0\n33.5\n1467.0\n282.0\n576.0\n0.093700\n0.862843\n0.024071\n0.019386\n0.245767\n1031\n0.15\n0.002908\n2063\nPOLYGON ((-8379626.770 4862662.870, -8379606.2...\n\n\n1\n42101037500\n3736.0\n31.3\n733.0\n55.0\n547.0\n0.447537\n0.392666\n0.079764\n0.080032\n0.632756\n486\n0.41\n0.022198\n901\nPOLYGON ((-8378518.360 4863306.140, -8378433.4...\n\n\n2\n42101021900\n1434.0\n44.7\n307.0\n0.0\n238.0\n0.911437\n0.034868\n0.028591\n0.025105\n0.166620\n167\n0.17\n0.176755\n826\nPOLYGON ((-8377278.820 4872712.160, -8377218.9...\n\n\n3\n42101011300\n3344.0\n28.5\n770.0\n333.0\n133.0\n0.018541\n0.965610\n0.015849\n0.000000\n0.067002\n975\n0.06\n0.000000\n1323\nPOLYGON ((-8375619.820 4863198.200, -8375613.5...\n\n\n4\n42101021800\n5141.0\n30.9\n1073.0\n92.0\n887.0\n0.548920\n0.291188\n0.034624\n0.125267\n0.597005\n490\n0.20\n0.037921\n712\nPOLYGON ((-8375464.090 4874647.550, -8375257.5...\n\n\n\n\n\n\n\n\nprint(final_aggregated_data.columns)\n\nIndex(['GEOID', 'Total Ho_1', 'NAME', 'state', 'county', 'tract', 'geometry',\n       'total_property_count', 'avg_rooms', 'avg_bathrooms', 'avg_bedrooms',\n       'avg_sale_price', 'avg_market_value', 'Total Popu_y', 'Median Age_y',\n       'White Alon_y', 'Black or A_y', 'Asian Alon_y', 'Hispanic o_y',\n       'Total Hous_y', 'Total Fami_y', 'Married-co_y', 'Poverty St_y',\n       'Below Pove_y', 'Median Hou_y', 'Occupied H_y', 'Vacant Hou_y',\n       'Owner-occu_y', 'Renter-occ_y'],\n      dtype='object')\n\n\n\ncensus_gdf = census_gdf.merge(\n    final_aggregated_data[[\"GEOID\",\"Total Popu_y\"]], \n    on=\"GEOID\", \n    how=\"left\"\n)\n\n\ncensus_gdf = census_gdf.merge(\n    final_aggregated_data[[\"GEOID\",'Median Age_y','Poverty St_y','Below Pove_y','Vacant Hou_y']], \n    on=\"GEOID\", \n    how=\"left\"\n)\n\n\ncensus_gdf.head()\n\n\n\n\n\n\n\n\nGEOID\nTotal Popu\nMedian Age\nPoverty St\nBelow Pove\nMarried-co\nwhite_ratio\nblack_ratio\nhisp_ratio\nother_ratio\n...\ntotal_crime_count\navg_sale_price\navg_rooms\ntotal_property_count\ngeometry\nTotal Popu_y\nMedian Age_y\nPoverty St_y\nBelow Pove_y\nVacant Hou_y\n\n\n\n\n0\n42101009802\n6190.0\n33.5\n1467.0\n282.0\n576.0\n0.093700\n0.862843\n0.024071\n0.019386\n...\n1031\n0.15\n0.002908\n2063\nPOLYGON ((-8379626.770 4862662.870, -8379606.2...\n6190.0\n33.5\n1467.0\n282.0\n242.0\n\n\n1\n42101037500\n3736.0\n31.3\n733.0\n55.0\n547.0\n0.447537\n0.392666\n0.079764\n0.080032\n...\n486\n0.41\n0.022198\n901\nPOLYGON ((-8378518.360 4863306.140, -8378433.4...\n3736.0\n31.3\n733.0\n55.0\n159.0\n\n\n2\n42101021900\n1434.0\n44.7\n307.0\n0.0\n238.0\n0.911437\n0.034868\n0.028591\n0.025105\n...\n167\n0.17\n0.176755\n826\nPOLYGON ((-8377278.820 4872712.160, -8377218.9...\n1434.0\n44.7\n307.0\n0.0\n97.0\n\n\n3\n42101011300\n3344.0\n28.5\n770.0\n333.0\n133.0\n0.018541\n0.965610\n0.015849\n0.000000\n...\n975\n0.06\n0.000000\n1323\nPOLYGON ((-8375619.820 4863198.200, -8375613.5...\n3344.0\n28.5\n770.0\n333.0\n290.0\n\n\n4\n42101021800\n5141.0\n30.9\n1073.0\n92.0\n887.0\n0.548920\n0.291188\n0.034624\n0.125267\n...\n490\n0.20\n0.037921\n712\nPOLYGON ((-8375464.090 4874647.550, -8375257.5...\n5141.0\n30.9\n1073.0\n92.0\n152.0\n\n\n\n\n5 rows × 21 columns\n\n\n\n\ncensus_gdf[\"crime_rate\"] = (census_gdf[\"total_crime_count\"] / census_gdf[\"Total Popu_y\"]) * 1000\ncensus_gdf[\"crime_rate\"] = census_gdf[\"crime_rate\"].fillna(0).replace([np.inf, -np.inf], 0)\n\n\ncensus_gdf = census_gdf[ (census_gdf[\"Total Popu_y\"] &gt; 0) & (census_gdf[\"crime_rate\"].notnull()) ]\n\n# Dependent V\ny = census_gdf[\"crime_rate\"]\n\n# independent V\nX = census_gdf[[\"black_ratio\", \"white_ratio\", \"avg_sale_price\"]]\n\n\nimport statsmodels.api as sm\n\n# I manually add the intercept here, it is constant for all variable so should have no infuence\nX = sm.add_constant(X)\nX.head()\n\n\n\n\n\n\n\n\nconst\nblack_ratio\nwhite_ratio\navg_sale_price\n\n\n\n\n0\n1.0\n0.862843\n0.093700\n0.15\n\n\n1\n1.0\n0.392666\n0.447537\n0.41\n\n\n2\n1.0\n0.034868\n0.911437\n0.17\n\n\n3\n1.0\n0.965610\n0.018541\n0.06\n\n\n4\n1.0\n0.291188\n0.548920\n0.20\n\n\n\n\n\n\n\n\nmodel = sm.OLS(y, X, missing='drop')\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             crime_rate   R-squared:                       0.003\nModel:                            OLS   Adj. R-squared:                 -0.005\nMethod:                 Least Squares   F-statistic:                    0.3550\nDate:                Thu, 26 Dec 2024   Prob (F-statistic):              0.786\nTime:                        17:02:08   Log-Likelihood:                -3156.2\nNo. Observations:                 376   AIC:                             6320.\nDf Residuals:                     372   BIC:                             6336.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nconst            574.9899    238.005      2.416      0.016     106.987    1042.993\nblack_ratio     -267.0188    295.893     -0.902      0.367    -848.852     314.814\nwhite_ratio     -236.9274    330.994     -0.716      0.475    -887.782     413.927\navg_sale_price     3.9940      9.458      0.422      0.673     -14.603      22.591\n==============================================================================\nOmnibus:                      810.129   Durbin-Watson:                   1.978\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          1125693.675\nSkew:                          15.633   Prob(JB):                         0.00\nKurtosis:                     269.224   Cond. No.                         51.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# X_no_const = X.drop(columns=\"const\", errors=\"ignore\") # statsmodels add_constant() 可能会加const\n# We do:\nX_no_const = X.loc[:, X.columns != 'const']\n\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X_no_const.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X_no_const.values, i) for i in range(X_no_const.shape[1])]\n\nprint(vif_data)\n\n          feature       VIF\n0     black_ratio  1.064220\n1     white_ratio  1.072523\n2  avg_sale_price  1.011484\n\n\n\nimport matplotlib.pyplot as plt\n\ny_pred = results.predict(X)\nplt.scatter(y, y_pred, alpha=0.5)\nplt.xlabel(\"Actual Crime Rate\")\nplt.ylabel(\"Predicted Crime Rate\")\nplt.title(\"OLS Fit: Actual vs. Predicted\")\nplt.show()\n\ncensus_gdf[\"resid\"] = results.resid\n\n\n\n\n\n\n\n\n\n#lets do some improvement\ncensus_gdf[\"Median Age_y\"] = census_gdf[\"Median Age_y\"].fillna(0)\ncensus_gdf[\"Poverty St_y\"] = census_gdf[\"Poverty St_y\"].fillna(0)\ncensus_gdf[\"Below Pove_y\"] = census_gdf[\"Below Pove_y\"].fillna(0)\ncensus_gdf[\"Vacant Hou_y\"] = census_gdf[\"Vacant Hou_y\"].fillna(0)\n\ncensus_gdf[\"below_poverty_ratio\"] = (\n    census_gdf[\"Below Pove_y\"] / census_gdf[\"Total Popu_y\"]\n).replace([np.inf, -np.inf], 0).fillna(0)\n\n\nfeature_cols = [\n    \"black_ratio\",\n    \"white_ratio\",\n    \"avg_sale_price\", \n    \"Median Age_y\",\n    \"below_poverty_ratio\",\n    \"Vacant Hou_y\"\n]\n\n\nimport statsmodels.api as sm\n\n# same y\ny = census_gdf[\"crime_rate\"]\n\n# new independent\nX = census_gdf[feature_cols].copy()\nX = X.fillna(0)\n\n# make a log to price to avoid extreme values\nX[\"avg_sale_price\"] = np.log1p(X[\"avg_sale_price\"])\n\nX = sm.add_constant(X)\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nX_no_const = X.drop(columns=[\"const\"], errors=\"ignore\")\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X_no_const.columns\nvif_data[\"VIF\"] = [\n    variance_inflation_factor(X_no_const.values, i) \n    for i in range(X_no_const.shape[1])\n]\nprint(vif_data)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             crime_rate   R-squared:                       0.050\nModel:                            OLS   Adj. R-squared:                  0.035\nMethod:                 Least Squares   F-statistic:                     3.237\nDate:                Thu, 26 Dec 2024   Prob (F-statistic):            0.00411\nTime:                        17:02:08   Log-Likelihood:                -3147.1\nNo. Observations:                 376   AIC:                             6308.\nDf Residuals:                     369   BIC:                             6336.\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n=======================================================================================\n                          coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nconst                -582.6870    429.256     -1.357      0.175   -1426.783     261.409\nblack_ratio           189.6411    326.899      0.580      0.562    -453.178     832.460\nwhite_ratio           667.2105    427.683      1.560      0.120    -173.791    1508.212\navg_sale_price        281.4062    149.743      1.879      0.061     -13.051     575.863\nMedian Age_y            8.4286      8.189      1.029      0.304      -7.674      24.531\nbelow_poverty_ratio  1.151e+04   2816.750      4.086      0.000    5971.367     1.7e+04\nVacant Hou_y           -0.6743      0.421     -1.600      0.110      -1.503       0.154\n==============================================================================\nOmnibus:                      781.262   Durbin-Watson:                   1.971\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           896403.493\nSkew:                          14.465   Prob(JB):                         0.00\nKurtosis:                     240.445   Cond. No.                     1.41e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.41e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n               feature        VIF\n0          black_ratio   8.066012\n1          white_ratio   8.710505\n2       avg_sale_price   1.488647\n3         Median Age_y  19.326683\n4  below_poverty_ratio   3.727617\n5         Vacant Hou_y   4.233158\n\n\n\n#What a pity, basically we cant really say the crime is related with sales prices and race yet, but it does highly realtes with poverty rate, lets try delete highly VIF factor to see if there are improvements\n\n\nfeature_cols = [\n    \"black_ratio\",       \n    \"white_ratio\",       \n    \"avg_sale_price\",    \n    \"Median Age_y\",      # VIF≈19\n    \"below_poverty_ratio\",\n    \"Vacant Hou_y\"\n]\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndf = census_gdf.copy()\n\nX = df[feature_cols].fillna(0).copy()\n\nX = sm.add_constant(X)\n\nX_no_const = X.drop(columns=[\"const\"], errors=\"ignore\")\n\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X_no_const.columns\nvif_data[\"VIF\"] = [\n    variance_inflation_factor(X_no_const.values, i) for i in range(X_no_const.shape[1])\n]\nprint(\"Initial VIF:\\n\", vif_data)\n\nInitial VIF:\n                feature        VIF\n0          black_ratio   8.124570\n1          white_ratio   8.672783\n2       avg_sale_price   1.040092\n3         Median Age_y  19.298617\n4  below_poverty_ratio   3.655698\n5         Vacant Hou_y   4.147828\n\n\n\nfeature_cols_reduced = [\n    \"black_ratio\",\n    \"white_ratio\",\n    # \"Median Age_y\",  # NO MORE!\n    \"avg_sale_price\",\n    \"below_poverty_ratio\",\n    \"Vacant Hou_y\"\n]\n\nX2 = df[feature_cols_reduced].fillna(0).copy()\n\nX2 = sm.add_constant(X2)\nX2_no_const = X2.drop(columns=[\"const\"], errors=\"ignore\")\n\nvif_data2 = pd.DataFrame()\nvif_data2[\"feature\"] = X2_no_const.columns\nvif_data2[\"VIF\"] = [\n    variance_inflation_factor(X2_no_const.values, i) for i in range(X2_no_const.shape[1])\n]\nprint(\"VIF after dropping Median Age_y:\\n\", vif_data2)\n\nVIF after dropping Median Age_y:\n                feature       VIF\n0          black_ratio  3.544943\n1          white_ratio  1.481318\n2       avg_sale_price  1.034620\n3  below_poverty_ratio  3.007876\n4         Vacant Hou_y  4.123210\n\n\n\nX_final = df[feature_cols_reduced].fillna(0).copy()\n\nX_final[\"avg_sale_price\"] = np.log1p(X_final[\"avg_sale_price\"])\n\nX_final = sm.add_constant(X_final)\n\ny = df[\"crime_rate\"].fillna(0)\n\nmodel = sm.OLS(y, X_final, missing='drop')\nresults = model.fit()\nprint(results.summary())\n\nX_no_const_final = X_final.drop(columns=[\"const\"], errors=\"ignore\")\nvif_data_final = pd.DataFrame({\n    \"feature\": X_no_const_final.columns,\n    \"VIF\": [\n        variance_inflation_factor(X_no_const_final.values, i)\n        for i in range(X_no_const_final.shape[1])\n    ]\n})\nprint(\"Final VIF after removing high-collinearity vars:\\n\", vif_data_final)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             crime_rate   R-squared:                       0.047\nModel:                            OLS   Adj. R-squared:                  0.034\nMethod:                 Least Squares   F-statistic:                     3.672\nDate:                Thu, 26 Dec 2024   Prob (F-statistic):            0.00296\nTime:                        17:02:09   Log-Likelihood:                -3147.6\nNo. Observations:                 376   AIC:                             6307.\nDf Residuals:                     370   BIC:                             6331.\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n=======================================================================================\n                          coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nconst                -315.5345    341.930     -0.923      0.357    -987.904     356.835\nblack_ratio           267.0040    318.167      0.839      0.402    -358.639     892.647\nwhite_ratio           760.4843    418.006      1.819      0.070     -61.481    1582.450\navg_sale_price        268.6425    149.241      1.800      0.073     -24.824     562.109\nbelow_poverty_ratio  1.119e+04   2800.248      3.998      0.000    5688.368    1.67e+04\nVacant Hou_y           -0.7413      0.416     -1.781      0.076      -1.560       0.077\n==============================================================================\nOmnibus:                      785.338   Durbin-Watson:                   1.975\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           930284.806\nSkew:                          14.623   Prob(JB):                         0.00\nKurtosis:                     244.919   Cond. No.                     1.40e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.4e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\nFinal VIF after removing high-collinearity vars:\n                feature       VIF\n0          black_ratio  3.537142\n1          white_ratio  1.717726\n2       avg_sale_price  1.478665\n3  below_poverty_ratio  3.057857\n4         Vacant Hou_y  4.204546\n\n\n\n#it seems like housing price are far not the most important factor when influencing housing prices\n\n\n# Data Analysis Overview\n# - Integrated crime incident data at the Census Tract level with American Community Survey (ACS) demographic and socioeconomic variables.\n# - Included aggregated housing price indicators.\n# - Performed linear regression to explore relationships between crime rate and variables such as:\n#   1. Racial composition (e.g., Black/White ratio)\n#   2. Poverty level\n#   3. Housing prices in Philadelphia\n\n# Regression Process and Findings\n# 1. Initial Model:\n#    - Predictors: black_ratio, white_ratio, avg_sale_price\n#    - Results:\n#      - R^2 ≈ 0.003 (very low explained variance).\n#      - None of the coefficients were statistically significant.\n#      - No strong linear relationship found between race ratio and crime rate.\n\n# 2. Model Refinements:\n#    - Added more controls:\n#      - Median Age_y\n#      - below_poverty_ratio\n#      - Vacant Hou_y\n#    - Applied a log transformation to avg_sale_price.\n#    - Findings:\n#      - Poverty ratio emerged as the only strongly significant positive predictor (coefficient ≈ 1.15×10^4, p &lt; 0.001).\n#      - Poverty remains the key factor associated with higher crime rates.\n#      - Other variables (race ratios, housing prices) were not significant at the 5% level.\n#      - Overall explained variance increased slightly but remained low (R^2 ≈ 0.05).\n\n# 3. Multicollinearity Issues:\n#    - Including too many covariates (e.g., Median Age_y) inflated Variance Inflation Factor (VIF &gt; 10), indicating high collinearity.\n#    - Dropping or merging certain variables reduced VIF but did not substantially improve the model’s explanatory power.\n\n# Conclusions\n# 1. \"Race vs. Crime Rate\" Relationship:\n#    - Neither Black nor White ratio showed a statistically significant linear effect on crime rate.\n#    - After controlling for factors like poverty and housing, racial proportions do not appear to determine crime rates.\n\n# 2. Importance of Poverty:\n#    - Poverty remains a consistent and strong predictor of crime rate, overshadowing any direct racial effects in this model.\n\n# 3. Model Limitations:\n#    - Low overall fit (R^2 ≈ 0.05), indicating that many important factors influencing crime are not included in the model.\n#    - Missing factors might include:\n#      - Policing strategies\n#      - Educational resources\n#      - Historical neighborhood conditions\n#    - Suggestion: Use more advanced models or richer datasets to better capture nuanced influences on crime distribution.\n\n\n#Q2,Here we test the relateness of each category relative to saling and market price\n\n\nimport geopandas as gpd\nimport statsmodels.api as sm\n\nshp_path = \"final/Philadelphia_ACS_2023.shp\"\n\ntry:\n    tracts_gdf = gpd.read_file(shp_path)\n    print(f\"Loaded {len(tracts_gdf)} Census Tracts from {shp_path}\")\nexcept FileNotFoundError:\n    raise FileNotFoundError(f\"The file at {shp_path} does not exist. Please check the path.\")\n\nif tracts_gdf.crs != \"EPSG:4326\":\n    tracts_gdf = tracts_gdf.to_crs(epsg=4326)\n\ntracts_gdf.head()\n\nLoaded 384 Census Tracts from final/Philadelphia_ACS_2023.shp\n\n\n\n\n\n\n\n\n\nGEOID\nTotal Popu\nMedian Age\nWhite Alon\nBlack or A\nAsian Alon\nHispanic o\nTotal Hous\nTotal Fami\nMarried-co\n...\nTotal Ho_1\nOccupied H\nVacant Hou\nOwner-occu\nRenter-occ\nNAME\nstate\ncounty\ntract\ngeometry\n\n\n\n\n0\n42101009802\n6190.0\n33.5\n580.0\n5341.0\n35.0\n149.0\n2129.0\n1467.0\n576.0\n...\n2371.0\n2129.0\n242.0\n1593.0\n536.0\nCensus Tract 98.02, Philadelphia County, Penns...\n42\n101\n009802\nPOLYGON ((-75.27547 39.97743, -75.27528 39.977...\n\n\n1\n42101037500\n3736.0\n31.3\n1672.0\n1467.0\n165.0\n298.0\n1224.0\n733.0\n547.0\n...\n1383.0\n1224.0\n159.0\n722.0\n502.0\nCensus Tract 375, Philadelphia County, Pennsyl...\n42\n101\n037500\nPOLYGON ((-75.26551 39.98186, -75.26475 39.982...\n\n\n2\n42101021900\n1434.0\n44.7\n1307.0\n50.0\n24.0\n41.0\n654.0\n307.0\n238.0\n...\n751.0\n654.0\n97.0\n551.0\n103.0\nCensus Tract 219, Philadelphia County, Pennsyl...\n42\n101\n021900\nPOLYGON ((-75.25438 40.04657, -75.25384 40.046...\n\n\n3\n42101011300\n3344.0\n28.5\n62.0\n3229.0\n0.0\n53.0\n1102.0\n770.0\n133.0\n...\n1392.0\n1102.0\n290.0\n621.0\n481.0\nCensus Tract 113, Philadelphia County, Pennsyl...\n42\n101\n011300\nPOLYGON ((-75.23947 39.98111, -75.23942 39.981...\n\n\n4\n42101021800\n5141.0\n30.9\n2822.0\n1497.0\n247.0\n178.0\n2242.0\n1073.0\n887.0\n...\n2394.0\n2242.0\n152.0\n597.0\n1645.0\nCensus Tract 218, Philadelphia County, Pennsyl...\n42\n101\n021800\nPOLYGON ((-75.23807 40.05988, -75.23622 40.061...\n\n\n\n\n5 rows × 23 columns\n\n\n\n\nprint(\"Number of crime records:\", len(crime_gdf))\ncrime_gdf.head()\n\ncrime_gdf = crime_gdf.to_crs(epsg=3857)\ntracts_gdf = tracts_gdf.to_crs(epsg=3857)\n\n# 1) sjoin\ncrime_by_tract = gpd.sjoin(\n    crime_gdf, \n    tracts_gdf, \n    how=\"inner\", \n    predicate=\"intersects\"\n).copy()\n\nprint(\"Records after sjoin:\", len(crime_by_tract))\ncrime_by_tract.head()\nprint(crime_by_tract.columns)\n\nNumber of crime records: 462960\nRecords after sjoin: 462145\nIndex(['cartodb_id', 'the_geom', 'the_geom_webmercator', 'objectid', 'dc_dist',\n       'psa', 'dispatch_date_time', 'dispatch_date', 'dispatch_time', 'hour',\n       'dc_key', 'location_block', 'ucr_general', 'text_general_code',\n       'point_x', 'point_y', 'geometry', 'year_month', 'crime_category',\n       'index_right', 'GEOID', 'Total Popu', 'Median Age', 'White Alon',\n       'Black or A', 'Asian Alon', 'Hispanic o', 'Total Hous', 'Total Fami',\n       'Married-co', 'Poverty St', 'Below Pove', 'Median Hou', 'Total Ho_1',\n       'Occupied H', 'Vacant Hou', 'Owner-occu', 'Renter-occ', 'NAME', 'state',\n       'county', 'tract'],\n      dtype='object')\n\n\n\ncrime_counts = (crime_by_tract\n    .groupby([\"GEOID\",\"crime_category\"])\n    .size()\n    .reset_index(name=\"count\")\n)\n\ncrime_counts.head()\n\n\n\n\n\n\n\n\nGEOID\ncrime_category\ncount\n\n\n\n\n0\n42101000100\nDrug/Alcohol Crime\n16\n\n\n1\n42101000100\nMiscellaneous\n419\n\n\n2\n42101000100\nOther\n274\n\n\n3\n42101000100\nProperty Crime\n1192\n\n\n4\n42101000100\nSexual Offenses\n8\n\n\n\n\n\n\n\n\ncrime_counts_wide = crime_counts.pivot_table(\n    index=\"GEOID\",\n    columns=\"crime_category\",\n    values=\"count\",\n    fill_value=0\n).reset_index()\n\ncrime_counts_wide.columns.name = None\ncrime_counts_wide.head()\n\n\n\n\n\n\n\n\nGEOID\nDrug/Alcohol Crime\nMiscellaneous\nOther\nProperty Crime\nSexual Offenses\nViolent Crime\n\n\n\n\n0\n42101000100\n16.0\n419.0\n274.0\n1192.0\n8.0\n138.0\n\n\n1\n42101000200\n12.0\n165.0\n174.0\n711.0\n24.0\n106.0\n\n\n2\n42101000300\n11.0\n413.0\n433.0\n942.0\n19.0\n128.0\n\n\n3\n42101000401\n3.0\n83.0\n94.0\n579.0\n3.0\n48.0\n\n\n4\n42101000402\n40.0\n601.0\n509.0\n1497.0\n46.0\n298.0\n\n\n\n\n\n\n\n\nprint(\"final_aggregated_data columns:\\n\", final_aggregated_data.columns)\nprint(\"Length of final_aggregated_data:\", len(final_aggregated_data))\n\nmerged_df = crime_counts_wide.merge(\n    final_aggregated_data, \n    on=\"GEOID\",\n    how=\"left\"\n)\n\nmerged_df = merged_df.fillna(0)\n\nprint(\"Merged shape:\", merged_df.shape)\nmerged_df.head()\n\nfinal_aggregated_data columns:\n Index(['GEOID', 'Total Ho_1', 'NAME', 'state', 'county', 'tract', 'geometry',\n       'total_property_count', 'avg_rooms', 'avg_bathrooms', 'avg_bedrooms',\n       'avg_sale_price', 'avg_market_value', 'Total Popu_y', 'Median Age_y',\n       'White Alon_y', 'Black or A_y', 'Asian Alon_y', 'Hispanic o_y',\n       'Total Hous_y', 'Total Fami_y', 'Married-co_y', 'Poverty St_y',\n       'Below Pove_y', 'Median Hou_y', 'Occupied H_y', 'Vacant Hou_y',\n       'Owner-occu_y', 'Renter-occ_y'],\n      dtype='object')\nLength of final_aggregated_data: 384\nMerged shape: (384, 35)\n\n\nC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_24168\\2015751945.py:10: DeprecationWarning: ExtensionArray.fillna added a 'copy' keyword in pandas 2.1.0. In a future version, ExtensionArray subclasses will need to implement this keyword or an exception will be raised. In the interim, the keyword is ignored by GeometryArray.\n  merged_df = merged_df.fillna(0)\n\n\n\n\n\n\n\n\n\nGEOID\nDrug/Alcohol Crime\nMiscellaneous\nOther\nProperty Crime\nSexual Offenses\nViolent Crime\nTotal Ho_1\nNAME\nstate\n...\nTotal Hous_y\nTotal Fami_y\nMarried-co_y\nPoverty St_y\nBelow Pove_y\nMedian Hou_y\nOccupied H_y\nVacant Hou_y\nOwner-occu_y\nRenter-occ_y\n\n\n\n\n0\n42101000100\n16.0\n419.0\n274.0\n1192.0\n8.0\n138.0\n2827.0\nCensus Tract 1, Philadelphia County, Pennsylvania\n42\n...\n2489.0\n753.0\n621.0\n753.0\n14.0\n103585.0\n2489.0\n338.0\n901.0\n1588.0\n\n\n1\n42101000200\n12.0\n165.0\n174.0\n711.0\n24.0\n106.0\n1219.0\nCensus Tract 2, Philadelphia County, Pennsylvania\n42\n...\n1082.0\n503.0\n402.0\n503.0\n55.0\n49871.0\n1082.0\n137.0\n523.0\n559.0\n\n\n2\n42101000300\n11.0\n413.0\n433.0\n942.0\n19.0\n128.0\n2063.0\nCensus Tract 3, Philadelphia County, Pennsylvania\n42\n...\n1828.0\n588.0\n516.0\n588.0\n16.0\n86296.0\n1828.0\n235.0\n441.0\n1387.0\n\n\n3\n42101000401\n3.0\n83.0\n94.0\n579.0\n3.0\n48.0\n1973.0\nCensus Tract 4.01, Philadelphia County, Pennsy...\n42\n...\n1618.0\n437.0\n414.0\n437.0\n50.0\n62986.0\n1618.0\n355.0\n491.0\n1127.0\n\n\n4\n42101000402\n40.0\n601.0\n509.0\n1497.0\n46.0\n298.0\n3448.0\nCensus Tract 4.02, Philadelphia County, Pennsy...\n42\n...\n2918.0\n672.0\n601.0\n672.0\n0.0\n78947.0\n2918.0\n530.0\n1679.0\n1239.0\n\n\n\n\n5 rows × 35 columns\n\n\n\n\ndef run_regression_crime_type(df, crime_col, housing_cols):\n    \"\"\"\n    df: merged dataframe\n    crime_col: e.g. \"Violent Crime\", \"Property Crime\", ...\n    housing_cols: list of columns to use as X\n    \"\"\"\n  \n    y = df[crime_col].values\n    \n   \n    X = df[housing_cols].copy()\n    \n    X = sm.add_constant(X, has_constant='add')\n    \n    model = sm.OLS(y, X, missing='drop')\n    results = model.fit()\n    print(f\"Regression on {crime_col} ~ {housing_cols}\")\n    print(results.summary())\n    print(\"-\"*80)\n    \n    return results\n    \nhousing_features = [\"avg_sale_price\", \"avg_rooms\", \"total_property_count\"]\n\n\ncrime_types = [\n    \"Violent Crime\",\n    \"Property Crime\",\n    \"Drug/Alcohol Crime\",\n    \"Sexual Offenses\",\n    \"Miscellaneous\"\n]\n\nresults_dict = {}\n\nfor ctype in crime_types:\n    if ctype in merged_df.columns:\n        results = run_regression_crime_type(\n            merged_df, \n            ctype, \n            housing_features\n        )\n        results_dict[ctype] = results\n    else:\n        print(f\"Column {ctype} not in merged_df, skipping...\")\n\nRegression on Violent Crime ~ ['avg_sale_price', 'avg_rooms', 'total_property_count']\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.205\nModel:                            OLS   Adj. R-squared:                  0.199\nMethod:                 Least Squares   F-statistic:                     32.64\nDate:                Thu, 26 Dec 2024   Prob (F-statistic):           8.60e-19\nTime:                        17:02:13   Log-Likelihood:                -2209.9\nNo. Observations:                 384   AIC:                             4428.\nDf Residuals:                     380   BIC:                             4444.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n========================================================================================\n                           coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nconst                   43.7517      9.639      4.539      0.000      24.799      62.704\navg_sale_price          -0.2682      0.647     -0.415      0.679      -1.540       1.003\navg_rooms              -55.5946     12.919     -4.303      0.000     -80.997     -30.192\ntotal_property_count     0.0463      0.005      8.456      0.000       0.036       0.057\n==============================================================================\nOmnibus:                      120.879   Durbin-Watson:                   0.782\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              399.855\nSkew:                           1.411   Prob(JB):                     1.49e-87\nKurtosis:                       7.127   Cond. No.                     5.77e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.77e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n--------------------------------------------------------------------------------\nRegression on Property Crime ~ ['avg_sale_price', 'avg_rooms', 'total_property_count']\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.084\nModel:                            OLS   Adj. R-squared:                  0.077\nMethod:                 Least Squares   F-statistic:                     11.67\nDate:                Thu, 26 Dec 2024   Prob (F-statistic):           2.48e-07\nTime:                        17:02:13   Log-Likelihood:                -2852.2\nNo. Observations:                 384   AIC:                             5712.\nDf Residuals:                     380   BIC:                             5728.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n========================================================================================\n                           coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nconst                  481.3326     51.328      9.378      0.000     380.410     582.255\navg_sale_price           0.2273      3.444      0.066      0.947      -6.544       6.999\navg_rooms             -200.7619     68.794     -2.918      0.004    -336.027     -65.497\ntotal_property_count     0.1431      0.029      4.909      0.000       0.086       0.200\n==============================================================================\nOmnibus:                      254.406   Durbin-Watson:                   1.620\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             2874.925\nSkew:                           2.688   Prob(JB):                         0.00\nKurtosis:                      15.280   Cond. No.                     5.77e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.77e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n--------------------------------------------------------------------------------\nRegression on Drug/Alcohol Crime ~ ['avg_sale_price', 'avg_rooms', 'total_property_count']\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.040\nModel:                            OLS   Adj. R-squared:                  0.033\nMethod:                 Least Squares   F-statistic:                     5.304\nDate:                Thu, 26 Dec 2024   Prob (F-statistic):            0.00136\nTime:                        17:02:13   Log-Likelihood:                -2257.8\nNo. Observations:                 384   AIC:                             4524.\nDf Residuals:                     380   BIC:                             4539.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n========================================================================================\n                           coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nconst                   -1.2540     10.919     -0.115      0.909     -22.724      20.216\navg_sale_price           0.0067      0.733      0.009      0.993      -1.434       1.447\navg_rooms              -20.9467     14.635     -1.431      0.153     -49.723       7.829\ntotal_property_count     0.0222      0.006      3.584      0.000       0.010       0.034\n==============================================================================\nOmnibus:                      615.491   Durbin-Watson:                   0.679\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           142748.835\nSkew:                           8.928   Prob(JB):                         0.00\nKurtosis:                      95.752   Cond. No.                     5.77e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.77e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n--------------------------------------------------------------------------------\nRegression on Sexual Offenses ~ ['avg_sale_price', 'avg_rooms', 'total_property_count']\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.025\nModel:                            OLS   Adj. R-squared:                  0.017\nMethod:                 Least Squares   F-statistic:                     3.202\nDate:                Thu, 26 Dec 2024   Prob (F-statistic):             0.0233\nTime:                        17:02:13   Log-Likelihood:                -1866.6\nNo. Observations:                 384   AIC:                             3741.\nDf Residuals:                     380   BIC:                             3757.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n========================================================================================\n                           coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nconst                    8.0772      3.942      2.049      0.041       0.326      15.829\navg_sale_price          -0.0336      0.265     -0.127      0.899      -0.554       0.487\navg_rooms              -10.2522      5.284     -1.940      0.053     -20.642       0.137\ntotal_property_count     0.0050      0.002      2.249      0.025       0.001       0.009\n==============================================================================\nOmnibus:                      806.172   Durbin-Watson:                   1.691\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          1063695.011\nSkew:                          14.805   Prob(JB):                         0.00\nKurtosis:                     259.133   Cond. No.                     5.77e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.77e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n--------------------------------------------------------------------------------\nRegression on Miscellaneous ~ ['avg_sale_price', 'avg_rooms', 'total_property_count']\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.167\nModel:                            OLS   Adj. R-squared:                  0.160\nMethod:                 Least Squares   F-statistic:                     25.37\nDate:                Thu, 26 Dec 2024   Prob (F-statistic):           5.58e-15\nTime:                        17:02:13   Log-Likelihood:                -2421.9\nNo. Observations:                 384   AIC:                             4852.\nDf Residuals:                     380   BIC:                             4868.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n========================================================================================\n                           coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nconst                   90.9089     16.741      5.430      0.000      57.992     123.826\navg_sale_price          -0.4549      1.123     -0.405      0.686      -2.664       1.754\navg_rooms              -75.0119     22.438     -3.343      0.001    -119.130     -30.894\ntotal_property_count     0.0729      0.010      7.670      0.000       0.054       0.092\n==============================================================================\nOmnibus:                      183.690   Durbin-Watson:                   1.059\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              965.740\nSkew:                           2.029   Prob(JB):                    1.96e-210\nKurtosis:                       9.625   Cond. No.                     5.77e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.77e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n--------------------------------------------------------------------------------\n\n\n\ncrime_types = [\"Violent Crime\",\"Property Crime\",\"Drug/Alcohol Crime\",\"Sexual Offenses\",\"Miscellaneous\"]\n\nfig, axes = plt.subplots(nrows=1, ncols=5, figsize=(20, 4), sharey=False)\n\nfor i, ctype in enumerate(crime_types):\n    ax = axes[i]\n    ax.scatter(merged_df[\"avg_sale_price\"], merged_df[ctype], alpha=0.5)\n    ax.set_xlabel(\"Avg Sale Price\")\n    ax.set_ylabel(ctype)\n    ax.set_title(f\"{ctype} vs. Avg Sale Price\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 6))\n\ncolors = [\"red\",\"blue\",\"green\",\"orange\",\"purple\"]\ncrime_types = [\"Violent Crime\",\"Property Crime\",\"Drug/Alcohol Crime\",\"Sexual Offenses\",\"Miscellaneous\"]\n\nfor ctype, color in zip(crime_types, colors):\n    plt.scatter(merged_df[\"avg_sale_price\"], merged_df[ctype], alpha=0.5, color=color, label=ctype)\n\nplt.xlabel(\"Avg Sale Price\")\nplt.ylabel(\"Crime Count\")\nplt.title(\"Crime Count vs. Avg Sale Price (All Types in One Plot)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Analysis Summary: Crime and Housing Predictors\n# \n# Linear regressions were run separately for different crime categories—Violent, Property, Drug/Alcohol, \n# Sexual Offenses, and Miscellaneous—using three housing-related predictors: avg_sale_price, avg_rooms, \n# and total_property_count. The goal was to examine whether basic housing characteristics could explain \n# variations in absolute crime counts.\n# \n# Violent Crime:\n# - Explanatory Power: R^2 ≈ 0.205 (highest among the models).\n# - Significant Predictors:\n#   - avg_rooms: negative and significant (areas with larger average rooms → fewer violent crimes).\n#   - total_property_count: strongly positive (larger property scale → higher violent crime counts).\n#   - avg_sale_price: insignificant.\n# \n# Property Crime:\n# - Explanatory Power: R^2 ≈ 0.084.\n# - Significant Predictors:\n#   - avg_rooms: negative and significant.\n#   - total_property_count: positive and significant.\n#   - avg_sale_price: no effect.\n# \n# Drug/Alcohol Crime:\n# - Explanatory Power: R^2 ≈ 0.040.\n# - Significant Predictors:\n#   - total_property_count: significantly positive.\n#   - avg_sale_price and avg_rooms: not significant.\n# \n# Sexual Offenses:\n# - Explanatory Power: R^2 ≈ 0.025 (lowest).\n# - Significant Predictors:\n#   - total_property_count: clearly positive.\n#   - avg_rooms: borderline significant (p = 0.053).\n#   - avg_sale_price: insignificant.\n# \n# Miscellaneous:\n# - Explanatory Power: R^2 ≈ 0.167.\n# - Significant Predictors:\n#   - avg_rooms: negative and significant.\n#   - total_property_count: positive and significant.\n#   - avg_sale_price: not significant.\n# \n# Overall Takeaways:\n# - Total Property Count consistently shows a positive effect across all models, indicating that areas with \n#   more properties (and potentially higher population or scale) tend to have higher absolute crime counts.\n# - Average Rooms often exhibits a negative association, especially for violent and property crimes, \n#   suggesting that areas with larger or more furnished homes might experience fewer such crimes.\n# - Sale Price never emerges as significant, indicating no clear linear relationship between a tract’s \n#   average home price and crime counts.\n# - Modest R^2 values (ranging from 2.5% to 20.5%) suggest that much of the variation in crime, particularly \n#   by type, is not explained by these basic housing metrics alone.\n\n\n#Q3 \n\n\n#lets have a peek of the data structure and data content\nprint(\"Columns in crime_gdf:\", crime_gdf.columns.tolist())\n\nprint(crime_gdf.head())\n\nColumns in crime_gdf: ['cartodb_id', 'the_geom', 'the_geom_webmercator', 'objectid', 'dc_dist', 'psa', 'dispatch_date_time', 'dispatch_date', 'dispatch_time', 'hour', 'dc_key', 'location_block', 'ucr_general', 'text_general_code', 'point_x', 'point_y', 'geometry', 'year_month', 'crime_category']\n   cartodb_id                                           the_geom  \\\n0           2  0101000020E6100000A51C8299A5C752C006342AD3DCFF...   \n1           4  0101000020E6100000F9245E3B64CC52C0B7195D940FF6...   \n2           7  0101000020E6100000118A52E7F6C052C0CFF41263190C...   \n3         123  0101000020E6100000E1F9FB7B5FC552C0159C0B6D4A02...   \n4         126  0101000020E6100000D1CCD5875CCA52C014B723FFC005...   \n\n                                the_geom_webmercator  objectid dc_dist psa  \\\n0  0101000020110F0000F80DE2A145E65FC1E5EC7592BE8F...       114      25   3   \n1  0101000020110F00000426B7CE54EE5FC1C5E06D37E284...       116      01   1   \n2  0101000020110F00006728CED7EBDA5FC169DB64F8519D...       119      08   2   \n3  0101000020110F00009D28D4D968E25FC13CD5C3D06F92...        96      15   1   \n4  0101000020110F00002F28E30AE2EA5FC10090A3314796...        99      14   1   \n\n     dispatch_date_time dispatch_date dispatch_time  hour        dc_key  \\\n0  2023-03-11T17:12:00Z    2023-03-11      12:12:00  12.0  202325014482   \n1  2023-03-11T18:31:00Z    2023-03-11      13:31:00  13.0  202301004597   \n2  2023-03-11T22:13:00Z    2023-03-11      17:13:00  17.0  202308008412   \n3  2023-03-11T12:42:00Z    2023-03-11      07:42:00   7.0  202315017366   \n4  2023-03-12T00:54:00Z    2023-03-11      19:54:00  19.0  202314012625   \n\n              location_block ucr_general   text_general_code    point_x  \\\n0    3300 BLOCK HARTVILLE ST         300  Robbery No Firearm -75.119482   \n1       2400 BLOCK S 28TH ST         600  Theft from Vehicle -75.193618   \n2  9800 BLOCK Roosevelt Blvd         600              Thefts -75.015070   \n3      4700 BLOCK GRISCOM ST         600              Thefts -75.083953   \n4        5500 BLOCK BLOYD ST         300  Robbery No Firearm -75.161898   \n\n     point_y                          geometry year_month  crime_category  \n0  39.998927  POINT (-8362262.529 4865786.288)    2023-03   Violent Crime  \n1  39.922350  POINT (-8370515.230 4854664.866)    2023-03  Property Crime  \n2  40.094525  POINT (-8350639.372 4879687.881)    2023-03  Property Crime  \n3  40.017896  POINT (-8358307.404 4868543.262)    2023-03  Property Crime  \n4  40.044952  POINT (-8366984.170 4872476.776)    2023-03   Violent Crime  \n\n\n\nunique_cats = crime_gdf[\"crime_category\"].unique()\nprint(\"Unique crime categories in crime_gdf:\", unique_cats)\n\nmissing_cats = set([\"Violent Crime\",\"Property Crime\",\"Drug/Alcohol Crime\",\"Sexual Offenses\",\"Miscellaneous\"]) - set(unique_cats)\nif missing_cats:\n    print(\"Warning: These categories not found in data:\", missing_cats)\nelse:\n    print(\"All 5 categories are present!\")\n\nUnique crime categories in crime_gdf: ['Violent Crime' 'Property Crime' 'Sexual Offenses' 'Other'\n 'Drug/Alcohol Crime' 'Miscellaneous']\nAll 5 categories are present!\n\n\n\n# convert type\ncrime_gdf[\"year_month\"] = crime_gdf[\"dispatch_date\"].dt.to_period(\"M\").astype(str)\n\ncrime_gdf[\"day_of_week\"] = crime_gdf[\"dispatch_date\"].dt.dayofweek\n\n# hour (which does not exist)\ncrime_gdf[\"hour_of_day\"] = crime_gdf[\"dispatch_date\"].dt.hour\n\n# days\nprint(\"Unique day_of_week:\", sorted(crime_gdf[\"day_of_week\"].unique()))\nprint(\"Unique hour_of_day:\", sorted(crime_gdf[\"hour_of_day\"].unique()))\n\nprint(\"Total records in crime_gdf:\", len(crime_gdf))\n\nmissing_date = crime_gdf[\"dispatch_date\"].isna().sum()\nprint(\"Missing dispatch_date rows:\", missing_date)\n\nmissing_cat = crime_gdf[\"crime_category\"].isna().sum()\nprint(\"Missing crime_category rows:\", missing_cat)\n\ngroup_month_cat = crime_gdf.groupby([\"year_month\",\"crime_category\"]).size().reset_index(name=\"count\")\nprint(\"First few lines of month-cat pivot:\")\nprint(group_month_cat.head(10).to_string())\n\n# Month\nunique_months = group_month_cat[\"year_month\"].unique()\nprint(f\"Found {len(unique_months)} distinct year_month values.\")\n\nUnique day_of_week: [0, 1, 2, 3, 4, 5, 6]\nUnique hour_of_day: [0]\nTotal records in crime_gdf: 462960\nMissing dispatch_date rows: 0\nMissing crime_category rows: 0\nFirst few lines of month-cat pivot:\n  year_month      crime_category  count\n0    2022-01  Drug/Alcohol Crime    257\n1    2022-01       Miscellaneous   1542\n2    2022-01               Other   1752\n3    2022-01      Property Crime   5608\n4    2022-01     Sexual Offenses    123\n5    2022-01       Violent Crime   1234\n6    2022-02  Drug/Alcohol Crime    335\n7    2022-02       Miscellaneous   1748\n8    2022-02               Other   1840\n9    2022-02      Property Crime   5295\nFound 36 distinct year_month values.\n\n\n\ngroup_dow_cat = crime_gdf.groupby([\"day_of_week\",\"crime_category\"]).size().reset_index(name=\"count\")\nprint(group_dow_cat.head(14))\n\ngroup_hour_cat = crime_gdf.groupby([\"hour_of_day\",\"crime_category\"]).size().reset_index(name=\"count\")\nprint(group_hour_cat.head(10))\n\ngroup_hour_cat = crime_gdf.groupby([\"hour_of_day\",\"crime_category\"]).size().reset_index(name=\"count\")\nprint(group_hour_cat.head(10))\n\n    day_of_week      crime_category  count\n0             0  Drug/Alcohol Crime   1184\n1             0       Miscellaneous  10568\n2             0               Other  11818\n3             0      Property Crime  41281\n4             0     Sexual Offenses    806\n5             0       Violent Crime   6115\n6             1  Drug/Alcohol Crime   1830\n7             1       Miscellaneous  12314\n8             1               Other  11492\n9             1      Property Crime  39193\n10            1     Sexual Offenses    853\n11            1       Violent Crime   5773\n12            2  Drug/Alcohol Crime   2020\n13            2       Miscellaneous  12228\n   hour_of_day      crime_category   count\n0            0  Drug/Alcohol Crime   11239\n1            0       Miscellaneous   72893\n2            0               Other   76261\n3            0      Property Crime  256726\n4            0     Sexual Offenses    5416\n5            0       Violent Crime   40425\n   hour_of_day      crime_category   count\n0            0  Drug/Alcohol Crime   11239\n1            0       Miscellaneous   72893\n2            0               Other   76261\n3            0      Property Crime  256726\n4            0     Sexual Offenses    5416\n5            0       Violent Crime   40425\n\n\n\n#good! It seems we have month and week level data, tho not hours, but this is enough to investigate\n\n\nmonthly_counts = crime_gdf.groupby([\"year_month\",\"crime_category\"]).size().reset_index(name=\"count\")\n\nweekly_counts = crime_gdf.groupby([\"day_of_week\",\"crime_category\"]).size().reset_index(name=\"count\")\n\n\nimport altair as alt\n\ndef create_time_charts(selected_category):\n    if selected_category == \"All\":\n        df_month = monthly_counts.groupby(\"year_month\")[\"count\"].sum().reset_index()\n        df_week = weekly_counts.groupby(\"day_of_week\")[\"count\"].sum().reset_index()\n    else:\n        df_month = monthly_counts[monthly_counts[\"crime_category\"]==selected_category]\n        df_week = weekly_counts[weekly_counts[\"crime_category\"]==selected_category]\n\n    month_chart = (\n        alt.Chart(df_month)\n        .mark_line(point=True)\n        .encode(\n            x=alt.X(\"year_month:N\", sort=None, title=\"Year-Month\"),\n            y=alt.Y(\"count:Q\", title=\"Crime Count\"),\n            tooltip=[\"year_month\",\"count\"]\n        )\n        .properties(\n            width=600,\n            height=300,\n            title=f\"Monthly Trend - {selected_category}\"\n        )\n    )\n    day_map = {0:\"Mon\",1:\"Tue\",2:\"Wed\",3:\"Thu\",4:\"Fri\",5:\"Sat\",6:\"Sun\"}\n    df_week[\"day_name\"] = df_week[\"day_of_week\"].map(day_map)\n\n    week_chart = (\n        alt.Chart(df_week)\n        .mark_bar()\n        .encode(\n            x=alt.X(\"day_name:N\", sort=[\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]),\n            y=alt.Y(\"count:Q\", title=\"Crime Count\"),\n            tooltip=[\"day_name\",\"count\"]\n        )\n        .properties(\n            width=300,\n            height=300,\n            title=f\"Weekly Distribution - {selected_category}\"\n        )\n    )\n\n    return month_chart, week_chart\n\n\nimport panel as pn\n\ncrime_types_all = [\"All\"] + sorted(crime_gdf[\"crime_category\"].unique().tolist())\n\ncategory_selector = pn.widgets.Select(\n    name=\"Crime Category\",\n    options=crime_types_all,\n    value=\"All\"\n)\n\n@pn.depends(category_selector.param.value)\ndef update_charts(selected_category):\n    mchart, wchart = create_time_charts(selected_category)\n    return pn.Row(\n        pn.pane.Vega(mchart, width=650, height=350),\n        pn.pane.Vega(wchart, width=350, height=350)\n    )\n\ntime_dashboard = pn.Column(\n    \"# Crime Monthly & Weekly Distribution\",\n    \"Select a crime category to see monthly trend & weekly distribution\",\n    category_selector,\n    update_charts\n)\n\ntime_dashboard.servable()\n\n\n\n\n\n  \n\n\n\n\n\nimport holoviews as hv\nimport hvplot.pandas\nhv.extension('bokeh')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\ncrime_gdf.crs = \"EPSG:3857\"\n\ncrime_gdf = crime_gdf[crime_gdf.geometry.notnull()]\n\ncrime_gdf[\"x\"] = crime_gdf.geometry.x.astype(float)\ncrime_gdf[\"y\"] = crime_gdf.geometry.y.astype(float)\n\ncrime_gdf = crime_gdf.replace([np.inf, -np.inf], np.nan)\ncrime_gdf = crime_gdf.dropna(subset=[\"x\",\"y\"])\n\ncrime_gdf.geometry.head()\n\n0    POINT (-8362262.529 4865786.288)\n1    POINT (-8370515.230 4854664.866)\n2    POINT (-8350639.372 4879687.881)\n3    POINT (-8358307.404 4868543.262)\n4    POINT (-8366984.170 4872476.776)\nName: geometry, dtype: geometry\n\n\n\ncrime_gdf.head(20)\n\n\n\n\n\n\n\n\ncartodb_id\nthe_geom\nthe_geom_webmercator\nobjectid\ndc_dist\npsa\ndispatch_date_time\ndispatch_date\ndispatch_time\nhour\n...\ntext_general_code\npoint_x\npoint_y\ngeometry\nyear_month\ncrime_category\nday_of_week\nhour_of_day\nx\ny\n\n\n\n\n0\n2\n0101000020E6100000A51C8299A5C752C006342AD3DCFF...\n0101000020110F0000F80DE2A145E65FC1E5EC7592BE8F...\n114\n25\n3\n2023-03-11T17:12:00Z\n2023-03-11\n12:12:00\n12.0\n...\nRobbery No Firearm\n-75.119482\n39.998927\nPOINT (-8362262.529 4865786.288)\n2023-03\nViolent Crime\n5\n0\n-8.362263e+06\n4.865786e+06\n\n\n1\n4\n0101000020E6100000F9245E3B64CC52C0B7195D940FF6...\n0101000020110F00000426B7CE54EE5FC1C5E06D37E284...\n116\n01\n1\n2023-03-11T18:31:00Z\n2023-03-11\n13:31:00\n13.0\n...\nTheft from Vehicle\n-75.193618\n39.922350\nPOINT (-8370515.230 4854664.866)\n2023-03\nProperty Crime\n5\n0\n-8.370515e+06\n4.854665e+06\n\n\n2\n7\n0101000020E6100000118A52E7F6C052C0CFF41263190C...\n0101000020110F00006728CED7EBDA5FC169DB64F8519D...\n119\n08\n2\n2023-03-11T22:13:00Z\n2023-03-11\n17:13:00\n17.0\n...\nThefts\n-75.015070\n40.094525\nPOINT (-8350639.372 4879687.881)\n2023-03\nProperty Crime\n5\n0\n-8.350639e+06\n4.879688e+06\n\n\n3\n123\n0101000020E6100000E1F9FB7B5FC552C0159C0B6D4A02...\n0101000020110F00009D28D4D968E25FC13CD5C3D06F92...\n96\n15\n1\n2023-03-11T12:42:00Z\n2023-03-11\n07:42:00\n7.0\n...\nThefts\n-75.083953\n40.017896\nPOINT (-8358307.404 4868543.262)\n2023-03\nProperty Crime\n5\n0\n-8.358307e+06\n4.868543e+06\n\n\n4\n126\n0101000020E6100000D1CCD5875CCA52C014B723FFC005...\n0101000020110F00002F28E30AE2EA5FC10090A3314796...\n99\n14\n1\n2023-03-12T00:54:00Z\n2023-03-11\n19:54:00\n19.0\n...\nRobbery No Firearm\n-75.161898\n40.044952\nPOINT (-8366984.170 4872476.776)\n2023-03\nViolent Crime\n5\n0\n-8.366984e+06\n4.872477e+06\n\n\n5\n128\n0101000020E61000002B7C851E94C252C0FB6A79ABCF03...\n0101000020110F000050B036BBA9DD5FC1A20DA3831F94...\n101\n15\n3\n2023-03-11T18:53:00Z\n2023-03-11\n13:53:00\n13.0\n...\nThefts\n-75.040290\n40.029775\nPOINT (-8353446.925 4870270.057)\n2023-03\nProperty Crime\n5\n0\n-8.353447e+06\n4.870270e+06\n\n\n6\n129\n0101000020E6100000C71530E485C852C03A8354C44800...\n0101000020110F000023CB499DC2E75FC1CD36223F3690...\n102\n25\n3\n2023-03-11T07:03:00Z\n2023-03-11\n02:03:00\n2.0\n...\nAggravated Assault Firearm\n-75.133172\n40.002221\nPOINT (-8363786.458 4866264.986)\n2023-03\nViolent Crime\n5\n0\n-8.363786e+06\n4.866265e+06\n\n\n7\n138\n0101000020E6100000E26C2165D7BE52C0EE405BD61608...\n0101000020110F000097CD95A350D75FC182830489DE98...\n110\n08\n2\n2023-03-11T12:22:00Z\n2023-03-11\n07:22:00\n7.0\n...\nTheft from Vehicle\n-74.981897\n40.063197\nPOINT (-8346946.556 4875130.141)\n2023-03\nProperty Crime\n5\n0\n-8.346947e+06\n4.875130e+06\n\n\n8\n146\n0101000020E6100000ABAA7E4249CF52C0CAFACDC474F6...\n0101000020110F000001F690843FF35FC18CE049475285...\n284\n12\n2\n2023-02-26T14:40:00Z\n2023-02-26\n09:40:00\n9.0\n...\nThefts\n-75.238846\n39.925439\nPOINT (-8375550.071 4855113.114)\n2023-02\nProperty Crime\n6\n0\n-8.375550e+06\n4.855113e+06\n\n\n9\n149\n0101000020E6100000DF2A99ADC6CE52C00BF379200DFD...\n0101000020110F0000EDBF38B661F25FC1DC5ECECBA08C...\n287\n19\n2\n2023-02-26T22:52:00Z\n2023-02-26\n17:52:00\n17.0\n...\nThefts\n-75.230876\n39.976963\nPOINT (-8374662.847 4862595.184)\n2023-02\nProperty Crime\n6\n0\n-8.374663e+06\n4.862595e+06\n\n\n10\n152\n0101000020E6100000093D93E496CF52C0EDE0D4C575FC...\n0101000020110F00007C0FB162C3F35FC1759C000EF98B...\n290\n19\n2\n2023-02-26T05:31:00Z\n2023-02-26\n00:31:00\n0.0\n...\nTheft from Vehicle\n-75.243585\n39.972344\nPOINT (-8376077.542 4861924.219)\n2023-02\nProperty Crime\n6\n0\n-8.376078e+06\n4.861924e+06\n\n\n11\n153\n0101000020E6100000D50648B048CB52C0E395DA41DBFC...\n0101000020110F000017AE3E2E73EC5FC137E49986698C...\n291\n22\n4\n2023-02-26T18:35:00Z\n2023-02-26\n13:35:00\n13.0\n...\nTheft from Vehicle\n-75.176312\n39.975441\nPOINT (-8368588.723 4862374.103)\n2023-02\nProperty Crime\n6\n0\n-8.368589e+06\n4.862374e+06\n\n\n12\n159\n0101000020E6100000E753850E13CC52C01D586D8298FD...\n0101000020110F000062125BECCAED5FC1E2F5A9473B8D...\n297\n22\n4\n2023-02-26T18:27:00Z\n2023-02-26\n13:27:00\n13.0\n...\nTheft from Vehicle\n-75.188663\n39.981217\nPOINT (-8369963.693 4863213.120)\n2023-02\nProperty Crime\n6\n0\n-8.369964e+06\n4.863213e+06\n\n\n13\n295\n0101000020E6100000DACF9CD4DBCB52C0E03C286A61F7...\n0101000020110F00007325B21D6DED5FC17D9F205F5886...\n213\n17\n2\n2023-02-26T05:46:00Z\n2023-02-26\n00:46:00\n0.0\n...\nAggravated Assault Firearm\n-75.185292\n39.932660\nPOINT (-8369588.464 4856161.486)\n2023-02\nViolent Crime\n6\n0\n-8.369588e+06\n4.856161e+06\n\n\n14\n298\n0101000020E6100000F9EBA1BFC8C952C05482518B39F5...\n0101000020110F0000A5891505E7E95FC1D8A22933F583...\n216\n03\n2\n2023-02-26T09:18:00Z\n2023-02-26\n04:18:00\n4.0\n...\nAggravated Assault No Firearm\n-75.152878\n39.915819\nPOINT (-8365980.079 4853716.799)\n2023-02\nViolent Crime\n6\n0\n-8.365980e+06\n4.853717e+06\n\n\n15\n300\n0101000020E6100000DE731E1DF0CA52C0A35E2A15D8F4...\n0101000020110F0000352233BADCEB5FC1FAC9FC478983...\n218\n03\n3\n2023-02-26T21:31:00Z\n2023-02-26\n16:31:00\n16.0\n...\nTheft from Vehicle\n-75.170905\n39.912844\nPOINT (-8367986.909 4853285.125)\n2023-02\nProperty Crime\n6\n0\n-8.367987e+06\n4.853285e+06\n\n\n16\n301\n0101000020E61000003AA2CE1E60CB52C09D4C9A0E3604...\n0101000020110F00009E3458FB9AEC5FC18ED7CF149194...\n219\n14\n3\n2023-02-26T21:23:00Z\n2023-02-26\n16:23:00\n16.0\n...\nThefts\n-75.177742\n40.032900\nPOINT (-8368747.927 4870724.325)\n2023-02\nProperty Crime\n6\n0\n-8.368748e+06\n4.870724e+06\n\n\n17\n303\n0101000020E61000009D593B1F58C952C0D52F09FED403...\n0101000020110F0000A1A859B627E95FC14653F26A2594...\n221\n35\n2\n2023-02-27T00:41:00Z\n2023-02-26\n19:41:00\n19.0\n...\nThefts\n-75.146004\n40.029938\nPOINT (-8365214.849 4870293.671)\n2023-02\nProperty Crime\n6\n0\n-8.365215e+06\n4.870294e+06\n\n\n18\n322\n0101000020E6100000A4A812E9E7CB52C041D910F397F5...\n0101000020110F0000B60B8DA281ED5FC1A7D720BD5D84...\n240\n01\n2\n2023-02-27T02:22:00Z\n2023-02-26\n21:22:00\n21.0\n...\nThefts\n-75.186030\n39.918700\nPOINT (-8369670.540 4854134.955)\n2023-02\nProperty Crime\n6\n0\n-8.369671e+06\n4.854135e+06\n\n\n19\n327\n0101000020E6100000812040860EC752C0172A628519FF...\n0101000020110F00008ADE100445E55FC132D1110EE68E...\n245\n24\n1\n2023-02-26T18:38:00Z\n2023-02-26\n13:38:00\n13.0\n...\nThefts\n-75.110261\n39.992966\nPOINT (-8361236.064 4864920.220)\n2023-02\nProperty Crime\n6\n0\n-8.361236e+06\n4.864920e+06\n\n\n\n\n20 rows × 23 columns\n\n\n\n\nimport holoviews as hv\nimport hvplot.pandas\nhv.extension('bokeh')\nimport datashader as ds\nimport numpy as np\n\n# months & cats\nmonths_sorted = sorted(crime_gdf[\"year_month\"].unique().tolist())\ncats_sorted   = sorted(crime_gdf[\"crime_category\"].unique().tolist())\n\ndf_hv = crime_gdf[[\"year_month\",\"crime_category\",\"x\",\"y\"]].copy()\n\n# monthly_counts for bar chart\nmonthly_counts = (\n    crime_gdf.groupby([\"year_month\",\"crime_category\"])\n             .size().reset_index(name=\"count\")\n)\n\n# color_key for multiple categories\ncolor_key_dict = {\n    \"Violent Crime\": \"#e31a1c\", \n    \"Property Crime\": \"#1f78b4\",\n    \"Drug/Alcohol Crime\":\"#33a02c\",\n    \"Sexual Offenses\": \"#ff7f00\",\n    \"Miscellaneous\":   \"#6a3d9a\",\n    \"Other\": \"#b15928\"\n}\n\nmonth_selector = pn.widgets.Select(\n    name=\"Month\",\n    options=months_sorted,\n    value=months_sorted[0]\n)\n\ncrime_selector = pn.widgets.Select(\n    name=\"Crime Category\",\n    options=[\"All\"] + cats_sorted,\n    value=\"Violent Crime\"\n)\n\ndef create_bar_chart(selected_month, selected_crime):\n    if selected_crime == \"All\":\n        chart_df = monthly_counts.groupby(\"year_month\", as_index=False)[\"count\"].sum()\n    else:\n        chart_df = monthly_counts[monthly_counts[\"crime_category\"]==selected_crime]\n\n    highlight = alt.condition(\n        alt.datum.year_month == selected_month,\n        alt.value(\"orange\"),  \n        alt.value(\"gray\")\n    )\n\n    bar = (\n        alt.Chart(chart_df)\n        .mark_bar()\n        .encode(\n            x=alt.X(\"year_month:N\", sort=None, title=\"Month\"),\n            y=alt.Y(\"count:Q\", title=\"Crime Count\"),\n            color=highlight,\n            tooltip=[\"year_month\",\"count\"]\n        )\n        .properties(\n            width=600,\n            height=250,\n            title=f\"Monthly Distribution for '{selected_crime}'\"\n        )\n    )\n    return bar\n\n@pn.depends(month_selector, crime_selector)\ndef update_map(selected_month, selected_crime):\n    if selected_crime == \"All\":\n        sub_df = df_hv[df_hv[\"year_month\"] == selected_month]\n        aggregator = ds.count_cat(\"crime_category\")\n        color_args = dict(color_key=color_key_dict)\n    else:\n        sub_df = df_hv[\n            (df_hv[\"year_month\"] == selected_month) &\n            (df_hv[\"crime_category\"] == selected_crime)\n        ]\n        aggregator = ds.count()\n        color_args = dict(cmap=[\"red\"])\n\n    if sub_df.empty:\n        return hv.Text(0,0,f\"No data for {selected_month}, {selected_crime}\").opts(\n            width=300, height=200\n        )\n\n    map_plot = sub_df.hvplot.points(\n        x=\"x\",\n        y=\"y\",\n        crs=\"EPSG:3857\",    \n        geo=True,\n        tiles=\"CartoLight\", \n        datashade=True,\n        dynspread=True,\n        aggregator=aggregator,\n        width=700,\n        height=450,\n        project=False,\n        **color_args   \n    )\n    return map_plot\n\n@pn.depends(month_selector, crime_selector)\ndef combined_dashboard(selected_month, selected_crime):\n    map_ = update_map(selected_month, selected_crime)\n    bar_chart = create_bar_chart(selected_month, selected_crime)\n    bar_pane = pn.pane.Vega(bar_chart, width=650, height=300)\n    return pn.Column(map_, bar_pane)\n\n\napp = pn.Column(\n    \"# Crime Distribution (Light Gray Map) with Legend & Bar Chart\",\n    pn.Row(month_selector, crime_selector),\n    combined_dashboard\n)\n\napp.servable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n# In this section, we focus on temporal patterns of crime across different\n# categories, examining both monthly (year_month) and weekly (day_of_week)\n# distributions. We utilize tools like Altair and a Panel-based dashboard\n# to visualize how various crime types shift over time.\n#\n# Monthly Patterns:\n#\n# Overall, crime tends to peak in summer (June–August), most notably\n# for violent crimes (e.g., aggravated assaults, robberies).\n# More ordinary/property crimes (like Thefts or Burglaries) show less\n# pronounced seasonal fluctuations, indicating a relatively steady\n# distribution throughout the year.\n#\n# Weekly/Day-of-Week Patterns:\n#\n# We observe certain types (e.g., drug/alcohol-related offenses)\n# spiking on weekend nights, whereas property or sexual offenses\n# appear more evenly spread.\n# Violent crime also exhibits a mild increase on late Friday/Saturday nights.\n#\n# Spatial Observations:\n#\n# From previous regressions involving poverty rates and minority\n# demographics, violent crimes align more strongly with higher-poverty\n# and higher-minority-population areas, and are amplified in summer months.\n# Property crimes appear more broadly dispersed, suggesting they are driven\n# by other factors (e.g., scale of housing) rather than strongly clustering\n# with demographic or economic disadvantage.\n#\n# Overall:\n#\n# Time dimension: A clear summer peak for violent crime, whereas\n# property/other crimes remain comparatively steady.\n# Spatial dimension: High-poverty/minority neighborhoods see elevated\n# violent crime counts, while property crimes do not exhibit such a strong\n# localized concentration.\n# These findings can help guide targeted public safety initiatives,\n# resource allocation, and seasonal policing strategies.",
    "crumbs": [
      "Background",
      "Project File.html"
    ]
  }
]